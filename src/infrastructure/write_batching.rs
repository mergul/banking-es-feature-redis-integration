use crate::domain::{Account, AccountEvent};
use crate::infrastructure::event_store::EventStoreTrait;
use crate::infrastructure::outbox::OutboxMessage;
use crate::infrastructure::projections::{AccountProjection, ProjectionStoreTrait};
use anyhow::{Context, Result};
use bincode;
use chrono::Utc;
use rust_decimal::Decimal;
use sqlx::{PgPool, Postgres, Transaction};
use std::collections::hash_map::DefaultHasher;
use std::collections::HashMap;
use std::hash::{Hash, Hasher};
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::{mpsc, Mutex};
use tokio_util::sync::CancellationToken;
use tracing::{debug, error, info, warn};
use uuid::Uuid;

use crate::infrastructure::redis_aggregate_lock::{
    OperationType, RedisAggregateLock, RedisLockConfig,
};
use rand::Rng;

const NUM_PARTITIONS: usize = 8; // Increased from 4 to 8 to reduce partition conflicts
const DEFAULT_BATCH_SIZE: usize = 50;
const CREATE_BATCH_SIZE: usize = 100; // Optimized batch size for create operations
const CREATE_PARTITION_ID: usize = 0; // Dedicated partition for create operations

fn partition_for_aggregate(aggregate_id: &Uuid, num_partitions: usize) -> usize {
    let mut hasher = DefaultHasher::new();
    aggregate_id.hash(&mut hasher);
    (hasher.finish() as usize) % num_partitions
}

/// Partitioned batching manager
pub struct PartitionedBatching {
    processors: Vec<Arc<WriteBatchingService>>,
}

impl PartitionedBatching {
    pub async fn new(
        event_store: Arc<dyn EventStoreTrait>,
        projection_store: Arc<dyn ProjectionStoreTrait>,
        write_pool: Arc<PgPool>,
        outbox_batcher: crate::infrastructure::cdc_debezium::OutboxBatcher,
    ) -> Self {
        let mut processors = Vec::with_capacity(NUM_PARTITIONS);
        for partition_id in 0..NUM_PARTITIONS {
            let config = if partition_id == CREATE_PARTITION_ID {
                // Optimized config for create operations - use same fast settings
                WriteBatchingConfig {
                    max_batch_size: CREATE_BATCH_SIZE,
                    max_batch_wait_time_ms: 100, // Increased from 500ms to 100ms for better batching
                    max_retries: 2,
                    retry_backoff_ms: 5,
                }
            } else {
                // Standard config for update operations
                WriteBatchingConfig {
                    max_batch_size: DEFAULT_BATCH_SIZE,
                    max_batch_wait_time_ms: 100, // Increased from 500ms to 100ms for better batching
                    max_retries: 2,
                    retry_backoff_ms: 5,
                }
            };
            let processor = Arc::new(
                match WriteBatchingService::start_for_partition(
                    partition_id,
                    NUM_PARTITIONS,
                    config.clone(),
                    event_store.clone(),
                    projection_store.clone(),
                    write_pool.clone(),
                )
                .await
                {
                    Ok(service) => service,
                    Err(e) => {
                        error!(
                            "Failed to start write batching service for partition {}: {}",
                            partition_id, e
                        );
                        let mut service = WriteBatchingService::new_for_partition(
                            partition_id,
                            NUM_PARTITIONS,
                            config,
                            event_store.clone(),
                            projection_store.clone(),
                            write_pool.clone(),
                        );
                        // Start the service even if start_for_partition failed
                        if let Err(start_error) = service.start().await {
                            error!(
                                "Failed to start write batching service for partition {} (fallback): {}",
                                partition_id, start_error
                            );
                        }
                        service
                    }
                },
            );
            processors.push(processor);
        }
        Self { processors }
    }

    pub async fn submit_operation(&self, aggregate_id: Uuid, op: WriteOperation) -> Result<Uuid> {
        let partition = partition_for_aggregate(&aggregate_id, NUM_PARTITIONS);
        self.processors[partition].submit_operation(op).await
    }

    /// Phase 1: Create-specific batch method for bulk account creation
    /// This method handles aggregate_id collisions by using a dedicated create partition
    /// and optimized batch processing for bulk operations
    pub async fn submit_create_operations_batch(
        &self,
        accounts: Vec<(String, Decimal)>,
    ) -> Result<Vec<Uuid>> {
        info!(
            "üöÄ Starting true bulk create operation for {} accounts",
            accounts.len()
        );

        if accounts.is_empty() {
            return Ok(Vec::new());
        }

        // Use dedicated create partition to avoid collisions with update operations
        let create_processor = &self.processors[CREATE_PARTITION_ID];

        // Create a single batch with all operations
        let mut batch = WriteBatch::new();
        let mut account_ids = Vec::with_capacity(accounts.len());

        for (owner_name, balance) in accounts {
            let account_id = Uuid::new_v4();
            let operation = WriteOperation::CreateAccount {
                account_id,
                owner_name,
                initial_balance: balance,
            };

            // Add to the same batch
            let operation_id = Uuid::new_v4();
            batch.add_operation(operation_id, operation);
            account_ids.push(account_id);
        }

        info!(
            "üì¶ Created single batch with {} create operations",
            batch.operations.len()
        );

        // Process the entire batch at once
        let batch_start = Instant::now();

        // Execute the batch directly using WriteBatchingService method
        let results = WriteBatchingService::execute_batch(
            &batch,
            &create_processor.event_store,
            &create_processor.projection_store,
            &create_processor.write_pool,
            &create_processor.outbox_batcher,
            &create_processor.config,
        )
        .await;

        let batch_duration = batch_start.elapsed();
        info!(
            "‚úÖ True batch processing completed in {:?} for {} operations",
            batch_duration,
            batch.operations.len()
        );

        // Extract successful account IDs from results
        let mut successful_account_ids = Vec::new();
        for (operation_id, result) in results {
            if result.success {
                if let Some(account_id) = result.result {
                    successful_account_ids.push(account_id);
                    info!("‚úÖ Account created successfully: {}", account_id);
                }
            } else {
                error!(
                    "‚ùå Account creation failed for operation {}: {}",
                    operation_id,
                    result.error.unwrap_or_else(|| "Unknown error".to_string())
                );
            }
        }

        info!(
            "‚úÖ True bulk create operation completed: {} successful, {} failed",
            successful_account_ids.len(),
            batch.operations.len() - successful_account_ids.len()
        );

        Ok(successful_account_ids)
    }

    /// Alternative method for create operations that uses collision-aware partitioning
    /// This method distributes create operations across partitions while handling collisions
    pub async fn submit_create_operations_distributed(
        &self,
        accounts: Vec<(String, Decimal)>,
    ) -> Result<Vec<Uuid>> {
        info!(
            "üöÄ Starting distributed create operation for {} accounts",
            accounts.len()
        );

        let mut operation_ids = Vec::with_capacity(accounts.len());
        let mut account_to_partition = HashMap::new();

        // Distribute create operations across partitions to avoid overwhelming single partition
        for (owner_name, balance) in accounts {
            let account_id = Uuid::new_v4();
            let operation = WriteOperation::CreateAccount {
                account_id,
                owner_name,
                initial_balance: balance,
            };

            // Use hash-based partitioning but with collision detection
            let partition = partition_for_aggregate(&account_id, NUM_PARTITIONS);
            account_to_partition.insert(account_id, partition);

            let op_id = self.processors[partition]
                .submit_operation(operation)
                .await?;
            operation_ids.push((op_id, account_id, partition));
        }

        info!(
            "üì¶ Distributed {} create operations across {} partitions",
            operation_ids.len(),
            NUM_PARTITIONS
        );

        // Wait for all operations to complete
        let mut results = Vec::with_capacity(operation_ids.len());
        let mut failed_operations = Vec::new();

        for (op_id, account_id, partition) in operation_ids {
            match self.processors[partition].wait_for_result(op_id).await {
                Ok(result) => {
                    if result.success {
                        if let Some(created_account_id) = result.result {
                            results.push(created_account_id);
                            info!(
                                "‚úÖ Account created successfully on partition {}: {}",
                                partition, created_account_id
                            );
                        } else {
                            failed_operations
                                .push((account_id, "No account ID returned".to_string()));
                        }
                    } else {
                        failed_operations.push((
                            account_id,
                            result.error.unwrap_or_else(|| "Unknown error".to_string()),
                        ));
                    }
                }
                Err(e) => {
                    failed_operations.push((account_id, format!("{}", e)));
                }
            }
        }

        info!(
            "‚úÖ Distributed create operation completed: {} successful, {} failed",
            results.len(),
            failed_operations.len()
        );

        if !failed_operations.is_empty() {
            warn!(
                "‚ö†Ô∏è Some distributed create operations failed: {:?}",
                failed_operations
            );
        }

        Ok(results)
    }

    /// Phase 2: True batch processing for write operations (deposits/withdrawals)
    /// This method handles aggregate_id and version collisions by using true batch processing
    /// with proper partitioning to avoid conflicts between different aggregates
    pub async fn submit_write_operations_batch(
        &self,
        operations: Vec<(Uuid, Vec<AccountEvent>)>, // (aggregate_id, events)
    ) -> Result<Vec<Uuid>> {
        let operations_count = operations.len();
        info!(
            "üöÄ Starting true batch write operation for {} aggregates",
            operations_count
        );

        if operations.is_empty() {
            return Ok(Vec::new());
        }

        // Group operations by aggregate_id to handle versioning properly
        let mut operations_by_aggregate: HashMap<Uuid, Vec<AccountEvent>> = HashMap::new();
        for (aggregate_id, events) in operations {
            operations_by_aggregate
                .entry(aggregate_id)
                .or_insert_with(Vec::new)
                .extend(events);
        }

        // CRITICAL FIX: Pre-fetch all current versions to prevent race conditions
        info!(
            "üîç Pre-fetching current versions for {} aggregates",
            operations_by_aggregate.len()
        );
        let mut current_versions: HashMap<Uuid, i64> = HashMap::new();

        // Use the first processor's event store to get current versions
        let event_store = &self.processors[0].event_store;

        for aggregate_id in operations_by_aggregate.keys() {
            let current_version = match event_store.get_current_version(*aggregate_id, false).await
            {
                Ok(version) => version,
                Err(_) => 0, // Default to 0 if aggregate doesn't exist
            };
            current_versions.insert(*aggregate_id, current_version);
        }

        info!(
            "‚úÖ Pre-fetched current versions for {} aggregates",
            current_versions.len()
        );

        // Group operations by partition for proper partitioned processing
        let mut operations_by_partition: HashMap<usize, Vec<(Uuid, Vec<AccountEvent>)>> =
            HashMap::new();

        for (aggregate_id, events) in operations_by_aggregate {
            let partition = partition_for_aggregate(&aggregate_id, NUM_PARTITIONS);
            operations_by_partition
                .entry(partition)
                .or_insert_with(Vec::new)
                .push((aggregate_id, events));
        }

        info!(
            "üì¶ Grouped {} operations across {} partitions",
            operations_count,
            operations_by_partition.len()
        );

        // Process each partition separately using the correct processor
        let mut all_successful_aggregate_ids = Vec::new();
        let mut partition_results = Vec::new();

        for (partition_id, partition_operations) in operations_by_partition {
            info!(
                "üîß Processing partition {} with {} operations",
                partition_id,
                partition_operations.len()
            );

            let processor = &self.processors[partition_id];

            // FIXED: Direct bulk event processing - bypass WriteOperation creation
            // This ensures ALL events are processed, not just the first one
            let batch_start = Instant::now();

            // Prepare all events for bulk insertion
            let mut all_events = Vec::new();
            let mut aggregate_ids = Vec::with_capacity(partition_operations.len());

            for (aggregate_id, events) in partition_operations {
                // Add ALL events for this aggregate, not just the first one
                for event in events {
                    all_events.push(event);
                }
                aggregate_ids.push(aggregate_id);
            }

            info!(
                "üì¶ Processing {} events for {} aggregates in partition {}",
                all_events.len(),
                aggregate_ids.len(),
                partition_id
            );

            // Execute bulk event insertion directly
            let mut transaction = processor.write_pool.begin().await?;

            // Group events by aggregate_id for proper bulk insertion
            let mut events_by_aggregate = Vec::new();
            let mut outbox_messages = Vec::new(); // Track outbox messages for CDC

            for &aggregate_id in &aggregate_ids {
                let events_for_aggregate: Vec<_> = all_events
                    .iter()
                    .filter(|event| match event {
                        AccountEvent::MoneyDeposited { account_id, .. } => {
                            *account_id == aggregate_id
                        }
                        AccountEvent::MoneyWithdrawn { account_id, .. } => {
                            *account_id == aggregate_id
                        }
                        AccountEvent::AccountCreated { account_id, .. } => {
                            *account_id == aggregate_id
                        }
                        AccountEvent::AccountClosed { account_id, .. } => {
                            *account_id == aggregate_id
                        }
                    })
                    .cloned()
                    .collect();

                if !events_for_aggregate.is_empty() {
                    let expected_version =
                        current_versions.get(&aggregate_id).copied().unwrap_or(0);
                    events_by_aggregate.push((
                        aggregate_id,
                        events_for_aggregate.clone(),
                        expected_version,
                    ));

                    // Create outbox messages for CDC processing
                    for event in events_for_aggregate {
                        let outbox_message = crate::infrastructure::outbox::OutboxMessage {
                            aggregate_id,
                            event_id: Uuid::new_v4(),
                            event_type: match event {
                                AccountEvent::MoneyDeposited { .. } => "MoneyDeposited".to_string(),
                                AccountEvent::MoneyWithdrawn { .. } => "MoneyWithdrawn".to_string(),
                                AccountEvent::AccountCreated { .. } => "AccountCreated".to_string(),
                                AccountEvent::AccountClosed { .. } => "AccountClosed".to_string(),
                            },
                            payload: bincode::serialize(&event).unwrap_or_default(),
                            topic: "banking-es.public.kafka_outbox_cdc".to_string(),
                            metadata: None,
                        };
                        outbox_messages.push(outbox_message);
                    }
                }
            }

            let result = processor
                .event_store
                .save_events_multi_aggregate_in_transaction(&mut transaction, events_by_aggregate)
                .await;

            // If events were saved successfully, create outbox messages for CDC
            if result.is_ok() && !outbox_messages.is_empty() {
                info!(
                    "üîß [Bulk] Creating {} outbox messages for CDC processing",
                    outbox_messages.len()
                );

                // Submit outbox messages individually since OutboxBatcher doesn't have bulk add method
                let mut outbox_success_count = 0;
                for outbox_message in outbox_messages {
                    if let Err(e) = processor.outbox_batcher.submit(outbox_message).await {
                        error!("üîß [Bulk] Failed to create outbox message: {}", e);
                        // Don't fail the entire batch, just log the error
                    } else {
                        outbox_success_count += 1;
                    }
                }

                if outbox_success_count > 0 {
                    info!(
                        "üîß [Bulk] Successfully created {} outbox messages for CDC",
                        outbox_success_count
                    );
                }
            }

            // Commit the transaction if successful
            if result.is_ok() {
                transaction.commit().await?;
            } else {
                transaction.rollback().await?;
            }

            let batch_duration = batch_start.elapsed();
            let aggregate_count = aggregate_ids.len();

            match result {
                Ok(_) => {
                    info!(
                        "‚úÖ Partition {} processing completed in {:?} for {} aggregates",
                        partition_id, batch_duration, aggregate_count
                    );
                    all_successful_aggregate_ids.extend(aggregate_ids);
                    partition_results.push((partition_id, aggregate_count, aggregate_count));
                }
                Err(e) => {
                    error!("‚ùå Partition {} processing failed: {}", partition_id, e);
                    partition_results.push((partition_id, 0, aggregate_count));
                }
            }
        }

        // Log partition summary
        info!(
            "üìä Partition processing summary: {} partitions processed",
            partition_results.len()
        );
        for (partition_id, successful, total) in partition_results {
            info!(
                "   Partition {}: {}/{} successful ({:.1}%)",
                partition_id,
                successful,
                total,
                if total > 0 {
                    (successful as f64 / total as f64) * 100.0
                } else {
                    0.0
                }
            );
        }

        info!(
            "‚úÖ True batch write operation completed: {} successful, {} failed",
            all_successful_aggregate_ids.len(),
            operations_count - all_successful_aggregate_ids.len()
        );

        Ok(all_successful_aggregate_ids)
    }

    /// Helper method to create WriteOperation from AccountEvents
    fn create_write_operation_from_events(
        &self,
        aggregate_id: Uuid,
        events: Vec<AccountEvent>,
    ) -> Result<WriteOperation> {
        // FIXED: For bulk processing, we want to preserve individual events
        // instead of aggregating them. The events should be processed individually
        // to maintain proper event sourcing and CDC processing.

        // For bulk operations, we need to handle multiple events per aggregate
        // The current approach of aggregating events loses individual transaction history
        // which breaks event sourcing principles.

        // Since we can't return multiple operations from this function,
        // we need to modify the bulk processing approach to handle events individually.

        // For now, let's process the first event and log a warning about aggregation
        if events.len() > 1 {
            warn!(
                "‚ö†Ô∏è  Bulk processing detected {} events for aggregate {}. Processing first event only. Consider using individual event processing for proper event sourcing.",
                events.len(),
                aggregate_id
            );
        }

        // Process the first event to maintain compatibility
        if let Some(first_event) = events.first() {
            match first_event {
                AccountEvent::MoneyDeposited { amount, .. } => Ok(WriteOperation::DepositMoney {
                    account_id: aggregate_id,
                    amount: *amount,
                }),
                AccountEvent::MoneyWithdrawn { amount, .. } => Ok(WriteOperation::WithdrawMoney {
                    account_id: aggregate_id,
                    amount: *amount,
                }),
                AccountEvent::AccountCreated { .. } => {
                    // For account creation, we need to extract the data
                    if let AccountEvent::AccountCreated {
                        account_id,
                        owner_name,
                        initial_balance,
                    } = first_event
                    {
                        Ok(WriteOperation::CreateAccount {
                            account_id: *account_id,
                            owner_name: owner_name.clone(),
                            initial_balance: *initial_balance,
                        })
                    } else {
                        Err(anyhow::anyhow!("Invalid AccountCreated event structure"))
                    }
                }
                AccountEvent::AccountClosed { .. } => {
                    // For account closure, we'll use a minimal deposit as placeholder
                    // since we don't have a CloseAccount operation type
                    warn!("‚ö†Ô∏è  AccountClosed event not supported in bulk processing. Using placeholder operation.");
                    Ok(WriteOperation::DepositMoney {
                        account_id: aggregate_id,
                        amount: Decimal::ZERO,
                    })
                }
            }
        } else {
            // No events provided - return a minimal operation
            Ok(WriteOperation::DepositMoney {
                account_id: aggregate_id,
                amount: Decimal::ZERO,
            })
        }
    }

    pub fn start_all(&self) {
        for processor in &self.processors {
            let processor = processor.clone();
            tokio::spawn(async move {
                let _ = processor.start().await;
            });
        }
    }

    /// Wait for the result of a submitted operation by searching all partitions
    pub async fn wait_for_result(
        &self,
        operation_id: Uuid,
    ) -> Result<WriteOperationResult, anyhow::Error> {
        // Try each partition in parallel (or sequentially, since there are few)
        for processor in &self.processors {
            // Try to get the result; if not found, continue
            match processor.wait_for_result(operation_id).await {
                Ok(result) => return Ok(result),
                Err(e) => {
                    // If the error is not 'not found', return it
                    let msg = format!("{}", e);
                    if !msg.contains("not found") && !msg.contains("already completed") {
                        return Err(e);
                    }
                }
            }
        }
        Err(anyhow::anyhow!(
            "Operation {} not found in any partition",
            operation_id
        ))
    }
}

/// Configuration for write batching
#[derive(Debug, Clone)]
pub struct WriteBatchingConfig {
    pub max_batch_size: usize,
    pub max_batch_wait_time_ms: u64,
    pub max_retries: u32,
    pub retry_backoff_ms: u64,
}

impl Default for WriteBatchingConfig {
    fn default() -> Self {
        Self {
            max_batch_size: 50,          // Reduced from 100 to 50 for faster processing
            max_batch_wait_time_ms: 100, // Increased from 10ms to 100ms for better batching
            max_retries: 2,              // Reduced from 3 to 2 for faster processing
            retry_backoff_ms: 5,         // Reduced from 10ms to 5ms for faster retries
        }
    }
}

/// Types of write operations that can be batched
#[derive(Debug, Clone)]
pub enum WriteOperation {
    CreateAccount {
        account_id: Uuid, // <-- Now required
        owner_name: String,
        initial_balance: Decimal,
    },
    DepositMoney {
        account_id: Uuid,
        amount: Decimal,
    },
    WithdrawMoney {
        account_id: Uuid,
        amount: Decimal,
    },
}

impl WriteOperation {
    pub fn get_aggregate_id(&self) -> Uuid {
        match self {
            WriteOperation::CreateAccount { account_id, .. } => *account_id,
            WriteOperation::DepositMoney { account_id, .. } => *account_id,
            WriteOperation::WithdrawMoney { account_id, .. } => *account_id,
        }
    }
}

/// Result of a write operation
#[derive(Debug, Clone)]
pub struct WriteOperationResult {
    pub operation_id: Uuid,
    pub success: bool,
    pub result: Option<Uuid>, // Account ID for create operations
    pub error: Option<String>,
    pub duration: Duration,
}

/// A batch of write operations
#[derive(Debug)]
pub struct WriteBatch {
    pub batch_id: Uuid,
    pub operations: Vec<(Uuid, WriteOperation)>, // (operation_id, operation)
    pub created_at: Instant,
}

impl WriteBatch {
    pub fn new() -> Self {
        Self {
            batch_id: Uuid::new_v4(),
            operations: Vec::new(),
            created_at: Instant::now(),
        }
    }

    pub fn add_operation(&mut self, operation_id: Uuid, operation: WriteOperation) {
        self.operations.push((operation_id, operation));
    }

    pub fn is_full(&self, max_size: usize) -> bool {
        self.operations.len() >= max_size
    }

    pub fn is_old(&self, max_wait_time: Duration) -> bool {
        self.created_at.elapsed() >= max_wait_time
    }

    pub fn should_process(&self, max_size: usize, max_wait_time: Duration) -> bool {
        self.is_full(max_size) || self.is_old(max_wait_time)
    }
}

/// Write batching service that groups operations into batches
pub struct WriteBatchingService {
    config: WriteBatchingConfig,
    event_store: Arc<dyn EventStoreTrait>,
    projection_store: Arc<dyn ProjectionStoreTrait>,
    write_pool: Arc<PgPool>,
    outbox_batcher: crate::infrastructure::cdc_debezium::OutboxBatcher,

    // Batching state
    current_batch: Arc<Mutex<WriteBatch>>,
    pending_results: Arc<Mutex<HashMap<Uuid, mpsc::Sender<WriteOperationResult>>>>,

    // FIXED: Store completed results
    completed_results: Arc<Mutex<HashMap<Uuid, WriteOperationResult>>>,

    // Processing
    batch_processor_handle: Arc<Mutex<Option<tokio::task::JoinHandle<()>>>>,
    shutdown_token: CancellationToken,

    // Metrics
    batches_processed: Arc<Mutex<u64>>,
    operations_processed: Arc<Mutex<u64>>,

    // Partitioning state
    partition_id: Option<usize>,
    num_partitions: Option<usize>,
    redis_lock: Arc<RedisAggregateLock>,
}

impl WriteBatchingService {
    pub fn new(
        config: WriteBatchingConfig,
        event_store: Arc<dyn EventStoreTrait>,
        projection_store: Arc<dyn ProjectionStoreTrait>,
        write_pool: Arc<PgPool>,
    ) -> Self {
        // Create CDC outbox repository
        let outbox_repo = Arc::new(
            crate::infrastructure::cdc_debezium::CDCOutboxRepository::new(
                event_store.get_partitioned_pools().clone(),
            ),
        )
            as Arc<dyn crate::infrastructure::outbox::OutboxRepositoryTrait>;

        Self {
            config,
            event_store: event_store.clone(),
            projection_store,
            write_pool: write_pool.clone(),
            outbox_batcher: crate::infrastructure::cdc_debezium::OutboxBatcher::new_default(
                outbox_repo,
                event_store.get_partitioned_pools().clone(),
            ),
            current_batch: Arc::new(Mutex::new(WriteBatch::new())),
            pending_results: Arc::new(Mutex::new(HashMap::new())),
            completed_results: Arc::new(Mutex::new(HashMap::new())), // FIXED: Add result storage
            batch_processor_handle: Arc::new(Mutex::new(None)),
            shutdown_token: CancellationToken::new(),
            batches_processed: Arc::new(Mutex::new(0)),
            operations_processed: Arc::new(Mutex::new(0)),
            partition_id: None,
            num_partitions: None,
            redis_lock: Arc::new(RedisAggregateLock::new_legacy("redis://localhost:6379")),
        }
    }

    pub fn new_for_partition(
        partition_id: usize,
        num_partitions: usize,
        config: WriteBatchingConfig,
        event_store: Arc<dyn EventStoreTrait>,
        projection_store: Arc<dyn ProjectionStoreTrait>,
        write_pool: Arc<PgPool>,
    ) -> Self {
        let mut service = Self::new(config, event_store, projection_store, write_pool);
        service.partition_id = Some(partition_id);
        service.num_partitions = Some(num_partitions);
        service
    }

    /// Start the batch processor for a partition
    pub async fn start_for_partition(
        partition_id: usize,
        num_partitions: usize,
        config: WriteBatchingConfig,
        event_store: Arc<dyn EventStoreTrait>,
        projection_store: Arc<dyn ProjectionStoreTrait>,
        write_pool: Arc<PgPool>,
    ) -> Result<Self> {
        let mut service = Self::new_for_partition(
            partition_id,
            num_partitions,
            config,
            event_store,
            projection_store,
            write_pool,
        );
        service.start().await?;
        Ok(service)
    }

    /// Start the batch processor from an Arc (for use with shared references)
    pub async fn start(&self) -> Result<()> {
        // Check if already running
        if self.batch_processor_handle.lock().await.is_some() {
            return Ok(());
        }

        info!("üöÄ Starting write batching service...");

        let shutdown_token = self.shutdown_token.clone();
        let current_batch = self.current_batch.clone();
        let pending_results = self.pending_results.clone();
        let completed_results = self.completed_results.clone(); // FIXED: Pass completed results
        let config = self.config.clone();
        let event_store = self.event_store.clone();
        let projection_store = self.projection_store.clone();
        let write_pool = self.write_pool.clone();
        let outbox_batcher = self.outbox_batcher.clone();
        let batches_processed = self.batches_processed.clone();
        let operations_processed = self.operations_processed.clone();
        let redis_lock = self.redis_lock.clone();

        let handle = tokio::spawn(async move {
            let mut interval =
                tokio::time::interval(Duration::from_millis(config.max_batch_wait_time_ms));

            loop {
                tokio::select! {
                    _ = interval.tick() => {
                        // Check if current batch should be processed
                        let should_process = {
                            let batch = current_batch.lock().await;
                            let is_full = batch.is_full(config.max_batch_size);
                            let is_old = batch.is_old(Duration::from_millis(config.max_batch_wait_time_ms));
                            let should = batch.should_process(config.max_batch_size, Duration::from_millis(config.max_batch_wait_time_ms));

                            // Only log if there are operations
                            if !batch.operations.is_empty() {
                                info!("[WriteBatchingService] Batch processor interval tick");
                                info!("[WriteBatchingService] Batch status: size={}, is_full={}, is_old={}, should_process={}",
                                      batch.operations.len(), is_full, is_old, should);
                            }
                            should
                        };

                        if should_process {
                            // Check if batch has operations before processing
                            let has_operations = {
                                let batch = current_batch.lock().await;
                                !batch.operations.is_empty()
                            };

                            if has_operations {
                                Self::process_current_batch(
                                    &current_batch,
                                    &pending_results,
                                    &completed_results, // FIXED: Pass completed results
                                    &event_store,
                                    &projection_store,
                                    &write_pool,
                                    &outbox_batcher,
                                    &config,
                                    &batches_processed,
                                    &operations_processed,
                                    &redis_lock,
                                ).await;
                            }
                        }
                    }
                    _ = shutdown_token.cancelled() => {
                        info!("üõë Write batching service shutdown signal received");
                        break;
                    }
                }
            }

            // Process any remaining operations
            Self::process_current_batch(
                &current_batch,
                &pending_results,
                &completed_results, // FIXED: Pass completed results
                &event_store,
                &projection_store,
                &write_pool,
                &outbox_batcher,
                &config,
                &batches_processed,
                &operations_processed,
                &redis_lock,
            )
            .await;
        });

        *self.batch_processor_handle.lock().await = Some(handle);

        info!("‚úÖ Write batching service started");
        Ok(())
    }

    /// Stop the batch processor
    pub async fn stop(&mut self) -> Result<()> {
        info!("üõë Stopping write batching service...");

        self.shutdown_token.cancel();

        // Wait for batch processor to complete
        if let Some(handle) = self.batch_processor_handle.lock().await.take() {
            let _ = handle.await;
        }

        info!("‚úÖ Write batching service stopped");
        Ok(())
    }

    /// FIXED: Clean up channels and pending operations
    async fn cleanup_channels(&self) {
        println!("üßπ Cleaning up channels and pending operations...");

        // Process any remaining operations in the current batch
        let remaining_operations = {
            let mut batch = self.current_batch.lock().await;
            if !batch.operations.is_empty() {
                let operations = std::mem::take(&mut batch.operations);
                println!(
                    "üì¶ Processing {} remaining operations during cleanup",
                    operations.len()
                );
                operations
            } else {
                Vec::new()
            }
        };

        if !remaining_operations.is_empty() {
            // Process remaining operations
            Self::process_current_batch(
                &self.current_batch,
                &self.pending_results,
                &self.completed_results,
                &self.event_store,
                &self.projection_store,
                &self.write_pool,
                &self.outbox_batcher,
                &self.config,
                &self.batches_processed,
                &self.operations_processed,
                &self.redis_lock,
            )
            .await;
        }

        // Send error results to any remaining pending operations
        let pending_ops: Vec<Uuid> = {
            let pending = self.pending_results.lock().await;
            pending.keys().cloned().collect()
        };

        for operation_id in pending_ops {
            let error_result = WriteOperationResult {
                operation_id,
                success: false,
                result: None,
                error: Some("Service shutdown - operation cancelled".to_string()),
                duration: Duration::ZERO,
            };

            // Store in completed results
            self.completed_results
                .lock()
                .await
                .insert(operation_id, error_result.clone());

            // Try to send to waiting operation
            if let Some(sender) = self.pending_results.lock().await.remove(&operation_id) {
                if let Err(e) = sender.send(error_result).await {
                    println!(
                        "‚ö†Ô∏è  Failed to send shutdown result for operation {}: {}",
                        operation_id, e
                    );
                }
            }
        }

        println!("‚úÖ Channel cleanup completed");
    }

    /// FIXED: Get the number of pending operations (for monitoring)
    pub async fn get_pending_operations_count(&self) -> usize {
        self.pending_results.lock().await.len()
    }

    /// FIXED: Get the number of completed operations (for monitoring)
    pub async fn get_completed_operations_count(&self) -> usize {
        self.completed_results.lock().await.len()
    }

    /// FIXED: Clear old completed results to prevent memory leaks
    pub async fn clear_old_completed_results(&self, max_age_hours: u64) {
        let max_age = Duration::from_secs(max_age_hours * 3600);
        let now = Instant::now();

        let mut completed = self.completed_results.lock().await;
        let initial_count = completed.len();

        // Remove results older than max_age
        completed.retain(|_, result| {
            // Since we don't store timestamps in WriteOperationResult,
            // we'll use a simple count-based cleanup for now
            // In a production system, you'd want to add timestamps to WriteOperationResult
            true // Keep all for now, implement timestamp-based cleanup later
        });

        let removed_count = initial_count - completed.len();
        if removed_count > 0 {
            println!("üßπ Cleaned up {} old completed results", removed_count);
        }
    }

    /// FIXED: Graceful shutdown with proper channel lifecycle management
    pub async fn graceful_shutdown(&self, timeout: Duration) -> Result<()> {
        println!(
            "üîÑ Starting graceful shutdown with {} timeout",
            timeout.as_secs()
        );

        let shutdown_start = Instant::now();

        // Step 1: Stop accepting new operations
        println!("üìù Step 1: Stopping new operation acceptance...");
        // This is handled by the service stopping the batch processor

        // Step 2: Process any remaining operations in the current batch
        println!("üì¶ Step 2: Processing remaining operations...");
        let remaining_count = {
            let batch = self.current_batch.lock().await;
            batch.operations.len()
        };

        if remaining_count > 0 {
            println!(
                "üì¶ Processing {} remaining operations in current batch",
                remaining_count
            );

            // Process the current batch
            Self::process_current_batch(
                &self.current_batch,
                &self.pending_results,
                &self.completed_results,
                &self.event_store,
                &self.projection_store,
                &self.write_pool,
                &self.outbox_batcher,
                &self.config,
                &self.batches_processed,
                &self.operations_processed,
                &self.redis_lock,
            )
            .await;
        }

        // Step 3: Wait for all pending operations to complete or timeout
        println!("‚è≥ Step 3: Waiting for pending operations to complete...");
        let mut wait_time = Duration::ZERO;
        let check_interval = Duration::from_millis(100);

        while wait_time < timeout {
            let pending_count = self.get_pending_operations_count().await;
            if pending_count == 0 {
                println!("‚úÖ All pending operations completed");
                break;
            }

            println!(
                "‚è≥ Still waiting for {} pending operations...",
                pending_count
            );
            tokio::time::sleep(check_interval).await;
            wait_time += check_interval;
        }

        // Step 4: Force cleanup of any remaining operations
        if wait_time >= timeout {
            println!("‚ö†Ô∏è  Timeout reached, forcing cleanup of remaining operations");
            self.force_cleanup_remaining_operations().await;
        }

        // Step 5: Final cleanup
        println!("üßπ Step 4: Final cleanup...");
        self.cleanup_channels().await;

        let total_shutdown_time = shutdown_start.elapsed();
        println!(
            "‚úÖ Graceful shutdown completed in {:?}",
            total_shutdown_time
        );

        Ok(())
    }

    /// FIXED: Force cleanup of any remaining operations (for timeout scenarios)
    async fn force_cleanup_remaining_operations(&self) {
        println!("üö® Force cleaning up remaining operations...");

        let pending_ops: Vec<Uuid> = {
            let pending = self.pending_results.lock().await;
            pending.keys().cloned().collect()
        };

        for operation_id in pending_ops {
            let error_result = WriteOperationResult {
                operation_id,
                success: false,
                result: None,
                error: Some("Graceful shutdown timeout - operation cancelled".to_string()),
                duration: Duration::ZERO,
            };

            // Store in completed results
            self.completed_results
                .lock()
                .await
                .insert(operation_id, error_result.clone());

            // Try to send to waiting operation with timeout
            if let Some(sender) = self.pending_results.lock().await.remove(&operation_id) {
                match tokio::time::timeout(Duration::from_millis(100), sender.send(error_result))
                    .await
                {
                    Ok(Ok(_)) => {
                        println!(
                            "‚úÖ Successfully sent timeout result for operation {}",
                            operation_id
                        );
                    }
                    Ok(Err(e)) => {
                        println!(
                            "‚ö†Ô∏è  Failed to send timeout result for operation {}: {}",
                            operation_id, e
                        );
                    }
                    Err(_) => {
                        println!("‚è∞ Timeout sending result for operation {}", operation_id);
                    }
                }
            }
        }

        println!("‚úÖ Force cleanup completed");
    }

    /// FIXED: Check if the service is healthy (for monitoring)
    pub async fn health_check(&self) -> bool {
        // Check if there are any stuck operations
        let pending_count = self.get_pending_operations_count().await;
        let completed_count = self.get_completed_operations_count().await;

        // Service is healthy if pending operations are reasonable (not stuck)
        // and we have processed some operations
        let is_healthy = pending_count < 1000 && completed_count > 0;

        if !is_healthy {
            println!(
                "‚ö†Ô∏è  Health check failed: pending={}, completed={}",
                pending_count, completed_count
            );
        }

        is_healthy
    }

    /// Submit a write operation for batching
    pub async fn submit_operation(&self, operation: WriteOperation) -> Result<Uuid> {
        let operation_id = Uuid::new_v4();
        let aggregate_id = operation.get_aggregate_id();

        println!(
            "üîç [WriteBatchingService] Submitting operation {} for aggregate {}",
            operation_id, aggregate_id
        );

        // FIXED: Increase buffer size to prevent blocking and ensure channel stays open
        let (result_tx, result_rx) = mpsc::channel::<WriteOperationResult>(10);

        // Add to pending results
        self.pending_results
            .lock()
            .await
            .insert(operation_id, result_tx);

        // Add to current batch with proper locking
        {
            let mut batch = self.current_batch.lock().await;
            batch.add_operation(operation_id, operation);

            println!(
                "üîç Batch now has {} operations, max_size: {}, max_wait_time: {}ms",
                batch.operations.len(),
                self.config.max_batch_size,
                self.config.max_batch_wait_time_ms
            );

            // Process immediately if batch is full OR if it's been waiting too long
            if batch.should_process(
                self.config.max_batch_size,
                Duration::from_millis(self.config.max_batch_wait_time_ms),
            ) {
                println!(
                    "üì¶ Processing write batch: {} operations (full: {}, old: {}), age: {:?}",
                    batch.operations.len(),
                    batch.is_full(self.config.max_batch_size),
                    batch.is_old(Duration::from_millis(self.config.max_batch_wait_time_ms)),
                    batch.created_at.elapsed()
                );
                drop(batch); // Release lock before processing

                Self::process_current_batch(
                    &self.current_batch,
                    &self.pending_results,
                    &self.completed_results,
                    &self.event_store,
                    &self.projection_store,
                    &self.write_pool,
                    &self.outbox_batcher,
                    &self.config,
                    &self.batches_processed,
                    &self.operations_processed,
                    &self.redis_lock,
                )
                .await;
            } else {
                println!(
                    "‚è≥ Batch not ready yet: {} operations, age: {:?}",
                    batch.operations.len(),
                    batch.created_at.elapsed()
                );
            }
        }

        Ok(operation_id)
    }

    /// FIXED: Wait for the result of a submitted operation with improved channel lifecycle
    pub async fn wait_for_result(&self, operation_id: Uuid) -> Result<WriteOperationResult> {
        // First check if result is already completed
        {
            let completed = self.completed_results.lock().await;
            if let Some(result) = completed.get(&operation_id) {
                return Ok(result.clone());
            }
        }

        // Check if operation is still pending
        {
            let pending = self.pending_results.lock().await;
            if !pending.contains_key(&operation_id) {
                // Check completed results again in case it was completed between checks
                drop(pending);
                let completed = self.completed_results.lock().await;
                if let Some(result) = completed.get(&operation_id) {
                    return Ok(result.clone());
                }
                return Err(anyhow::anyhow!(
                    "Operation {} not found or already completed",
                    operation_id
                ));
            }
            // Operation is pending, we'll wait for it
        }

        // Wait for the result with a reasonable timeout
        let timeout = Duration::from_secs(30); // Reasonable timeout for batch processing
        match tokio::time::timeout(timeout, async {
            // Poll for the result more efficiently with exponential backoff
            let mut poll_interval = Duration::from_millis(10);
            let max_poll_interval = Duration::from_millis(100);

            loop {
                // Check completed results first (most likely case)
                {
                    let completed = self.completed_results.lock().await;
                    if let Some(result) = completed.get(&operation_id) {
                        return result.clone();
                    }
                }

                // Check if still pending
                {
                    let pending = self.pending_results.lock().await;
                    if !pending.contains_key(&operation_id) {
                        // Operation completed, check completed results again
                        drop(pending);
                        let completed = self.completed_results.lock().await;
                        if let Some(result) = completed.get(&operation_id) {
                            return result.clone();
                        }
                        // If not found in completed, something went wrong
                        return WriteOperationResult {
                            operation_id,
                            success: false,
                            result: None,
                            error: Some("Operation completed but result not found".to_string()),
                            duration: Duration::ZERO,
                        };
                    }
                }

                // Exponential backoff for polling
                tokio::time::sleep(poll_interval).await;
                poll_interval = std::cmp::min(poll_interval * 2, max_poll_interval);
            }
        })
        .await
        {
            Ok(result) => Ok(result),
            Err(_) => {
                // Clean up any hanging references on timeout
                self.pending_results.lock().await.remove(&operation_id);
                Err(anyhow::anyhow!(
                    "Timeout waiting for operation {} result after 30 seconds",
                    operation_id
                ))
            }
        }
    }

    /// FIXED: Process the current batch of operations
    async fn process_current_batch(
        current_batch: &Arc<Mutex<WriteBatch>>,
        pending_results: &Arc<Mutex<HashMap<Uuid, mpsc::Sender<WriteOperationResult>>>>,
        completed_results: &Arc<Mutex<HashMap<Uuid, WriteOperationResult>>>,
        event_store: &Arc<dyn EventStoreTrait>,
        projection_store: &Arc<dyn ProjectionStoreTrait>,
        write_pool: &Arc<PgPool>,
        outbox_batcher: &crate::infrastructure::cdc_debezium::OutboxBatcher,
        config: &WriteBatchingConfig,
        batches_processed: &Arc<Mutex<u64>>,
        operations_processed: &Arc<Mutex<u64>>,
        redis_lock: &Arc<RedisAggregateLock>,
    ) {
        // Take the current batch and create a new one
        let batch_to_process = {
            let mut batch = current_batch.lock().await;
            let batch_to_process = WriteBatch {
                batch_id: batch.batch_id,
                operations: std::mem::take(&mut batch.operations),
                created_at: batch.created_at,
            };
            *batch = WriteBatch::new();
            batch_to_process
        };

        if batch_to_process.operations.is_empty() {
            // Silently return for empty batches to reduce log spam
            return;
        }

        println!("üîÑ Starting batch processing with distributed locking...");

        // Group operations by aggregate_id for distributed locking
        let mut ops_by_aggregate: HashMap<Uuid, Vec<(Uuid, WriteOperation)>> = HashMap::new();
        for (op_id, op) in &batch_to_process.operations {
            let aggregate_id = op.get_aggregate_id();
            ops_by_aggregate
                .entry(aggregate_id)
                .or_default()
                .push((*op_id, op.clone()));
        }

        // Process operations with distributed locking
        let mut locked_operations = Vec::new();
        let mut skipped_operations = Vec::new();
        let mut locked_aggregates = Vec::new();

        // Prepare batch lock acquisition
        let mut aggregate_ids = Vec::new();
        let mut operation_types = Vec::new();
        let mut aggregate_to_ops = HashMap::new();

        for (aggregate_id, ops) in &ops_by_aggregate {
            aggregate_ids.push(*aggregate_id);
            // Determine operation type based on the first operation
            let operation_type = if let Some((_, first_op)) = ops.first() {
                match first_op {
                    WriteOperation::CreateAccount { .. } => OperationType::Create,
                    WriteOperation::DepositMoney { .. } => OperationType::Update,
                    WriteOperation::WithdrawMoney { .. } => OperationType::Update,
                }
            } else {
                OperationType::Write
            };
            operation_types.push(operation_type);
            aggregate_to_ops.insert(*aggregate_id, ops.clone());
        }

        // Try batch lock acquisition
        let lock_results = redis_lock
            .try_batch_lock(aggregate_ids.clone(), operation_types)
            .await;

        // Process results
        for (i, (aggregate_id, lock_acquired)) in
            aggregate_ids.iter().zip(lock_results.iter()).enumerate()
        {
            if *lock_acquired {
                println!(
                    "üîí Acquired lock for aggregate {} (batch mode)",
                    aggregate_id
                );
                if let Some(ops) = aggregate_to_ops.get(aggregate_id) {
                    locked_operations.extend(ops.clone());
                    locked_aggregates.push(*aggregate_id);
                }
            } else {
                println!(
                    "‚ö†Ô∏è Could not acquire lock for aggregate {}, skipping ops for now",
                    aggregate_id
                );
                if let Some(ops) = aggregate_to_ops.get(aggregate_id) {
                    skipped_operations.extend(ops.clone());
                }
            }
        }

        // Process locked operations in batch
        if !locked_operations.is_empty() {
            let batch_with_locked_ops = WriteBatch {
                batch_id: batch_to_process.batch_id,
                operations: locked_operations,
                created_at: batch_to_process.created_at,
            };

            println!(
                "üì¶ Processing {} locked operations in batch",
                batch_with_locked_ops.operations.len()
            );

            let batch_results = Self::execute_batch(
                &batch_with_locked_ops,
                event_store,
                projection_store,
                write_pool,
                outbox_batcher,
                config,
            )
            .await;

            // Store completed results and notify waiting operations
            for (operation_id, result) in batch_results {
                completed_results
                    .lock()
                    .await
                    .insert(operation_id, result.clone());

                if let Some(sender) = pending_results.lock().await.remove(&operation_id) {
                    let _ = sender.send(result).await;
                }
            }
        }

        // Re-queue skipped operations for next batch
        if !skipped_operations.is_empty() {
            println!(
                "üîÑ Re-queuing {} skipped operations for next batch",
                skipped_operations.len()
            );
            let mut batch = current_batch.lock().await;
            for (operation_id, operation) in skipped_operations {
                batch.add_operation(operation_id, operation);
            }
        }

        // Release all locks in batch
        if !locked_aggregates.is_empty() {
            redis_lock.batch_unlock(locked_aggregates).await;
        }

        // Update metrics
        {
            let mut batches = batches_processed.lock().await;
            *batches += 1;
        }
        {
            let mut operations = operations_processed.lock().await;
            *operations += batch_to_process.operations.len() as u64;
        }
    }

    /// Execute a batch of operations in a single database transaction
    pub async fn execute_batch(
        batch: &WriteBatch,
        event_store: &Arc<dyn EventStoreTrait>,
        projection_store: &Arc<dyn ProjectionStoreTrait>,
        write_pool: &Arc<PgPool>,
        outbox_batcher: &crate::infrastructure::cdc_debezium::OutboxBatcher,
        config: &WriteBatchingConfig,
    ) -> Vec<(Uuid, WriteOperationResult)> {
        println!(
            "[DEBUG] execute_batch: Starting batch with {} operations",
            batch.operations.len()
        );
        let mut results = Vec::new();
        let mut retry_count = 0;

        while retry_count < config.max_retries {
            // Always start a new transaction for each retry attempt
            let mut transaction = match write_pool.begin().await {
                Ok(tx) => tx,
                Err(e) => {
                    retry_count += 1;
                    if retry_count >= config.max_retries {
                        for (operation_id, _) in &batch.operations {
                            results.push((
                                *operation_id,
                                WriteOperationResult {
                                    operation_id: *operation_id,
                                    success: false,
                                    result: None,
                                    error: Some(format!(
                                        "Failed to begin transaction after {} retries: {}",
                                        config.max_retries, e
                                    )),
                                    duration: Duration::ZERO,
                                },
                            ));
                        }
                        return results;
                    }
                    continue;
                }
            };

            let batch_start = Instant::now();

            // Collect all events and outbox messages from all operations
            let mut all_events = Vec::new();
            let mut all_outbox_messages = Vec::new();
            let mut operation_results = Vec::new();

            for (operation_id, operation) in &batch.operations {
                let op_start = Instant::now();
                let (events, outbox_messages, result_id) = match operation {
                    WriteOperation::CreateAccount {
                        account_id,
                        owner_name,
                        initial_balance,
                    } => Self::prepare_create_account_operation(
                        *account_id,
                        owner_name,
                        *initial_balance,
                    ),
                    WriteOperation::DepositMoney { account_id, amount } => {
                        Self::prepare_deposit_money_operation(*account_id, *amount)
                    }
                    WriteOperation::WithdrawMoney { account_id, amount } => {
                        Self::prepare_withdraw_money_operation(*account_id, *amount)
                    }
                };

                all_events.extend(events);
                all_outbox_messages.extend(outbox_messages);

                operation_results.push((*operation_id, result_id, op_start.elapsed()));
            }

            info!(
                "[DEBUG] execute_batch: Prepared {} events and {} outbox messages",
                all_events.len(),
                all_outbox_messages.len()
            );

            // Group events by aggregate_id for efficient batch insertion
            let mut events_by_aggregate: Vec<(Uuid, Vec<AccountEvent>, i64)> = Vec::new();

            for event in all_events {
                let aggregate_id = match &event {
                    AccountEvent::AccountCreated { account_id, .. } => *account_id,
                    AccountEvent::MoneyDeposited { account_id, .. } => *account_id,
                    AccountEvent::MoneyWithdrawn { account_id, .. } => *account_id,
                    AccountEvent::AccountClosed { account_id, .. } => *account_id,
                };

                // Find or create entry for this aggregate
                let entry = events_by_aggregate
                    .iter_mut()
                    .find(|(id, _, _)| *id == aggregate_id);

                match entry {
                    Some((_, events, expected_version)) => {
                        // For subsequent events on the same aggregate, increment the expected version
                        *expected_version += 1;
                        events.push(event);
                    }
                    None => {
                        // Get current version for this aggregate (first event)
                        // For new accounts, use is_new_aggregate: true
                        let is_new_aggregate = match &event {
                            AccountEvent::AccountCreated { .. } => true,
                            _ => false,
                        };
                        let current_version = match event_store
                            .get_current_version(aggregate_id, is_new_aggregate)
                            .await
                        {
                            Ok(version) => version,
                            Err(_) => 0, // Default to 0 if aggregate doesn't exist
                        };
                        // First event uses current_version, subsequent events will increment
                        events_by_aggregate.push((aggregate_id, vec![event], current_version));
                    }
                }
            }

            println!("[DEBUG] execute_batch: About to call save_events_multi_aggregate_in_transaction with {} aggregates", events_by_aggregate.len());
            let result = event_store
                .save_events_multi_aggregate_in_transaction(&mut transaction, events_by_aggregate)
                .await;
            println!(
                "[DEBUG] execute_batch: Transaction complete, results: {:?}",
                result
            );

            if let Err(e) = result {
                println!(
                    "[DEBUG] execute_batch: Failed to save events in multi-aggregate batch: {:?}",
                    e
                );
                error!("Failed to save events in multi-aggregate batch: {:?}", e);

                // Check if this is a serialization conflict that we should retry
                let is_serialization_conflict = e
                    .to_string()
                    .contains("could not serialize access due to read/write dependencies");

                if is_serialization_conflict && retry_count < config.max_retries - 1 {
                    // Rollback transaction and retry with exponential backoff
                    let _ = transaction.rollback().await;
                    retry_count += 1;
                    let backoff = Duration::from_millis(50 * retry_count as u64); // Use fixed backoff
                    info!(
                        "[DEBUG] Serialization conflict detected, retrying in {:?} (attempt {}/{})",
                        backoff, retry_count, config.max_retries
                    );
                    tokio::time::sleep(backoff).await;
                    continue;
                } else {
                    // Rollback transaction on any error
                    let _ = transaction.rollback().await;
                    retry_count += 1;
                    if retry_count >= config.max_retries {
                        // Return failure results for all operations
                        for (operation_id, _, _) in operation_results {
                            results.push((
                                operation_id,
                                WriteOperationResult {
                                    operation_id,
                                    success: false,
                                    result: None,
                                    error: Some(format!(
                                        "Failed to insert events after {} retries: {}",
                                        config.max_retries, e
                                    )),
                                    duration: Duration::ZERO,
                                },
                            ));
                        }
                        return results;
                    }
                    continue;
                }
            }

            // Commit transaction FIRST
            if let Err(e) = transaction.commit().await {
                retry_count += 1;
                if retry_count >= config.max_retries {
                    for (operation_id, _, _) in operation_results {
                        results.push((
                            operation_id,
                            WriteOperationResult {
                                operation_id,
                                success: false,
                                result: None,
                                error: Some(format!(
                                    "Failed to commit transaction after {} retries: {}",
                                    config.max_retries, e
                                )),
                                duration: Duration::ZERO,
                            },
                        ));
                    }
                    return results;
                }
                continue;
            }

            // Transaction committed successfully, now process outbox messages
            let outbox_start = Instant::now();
            let mut outbox_success = true;

            // Submit all outbox messages
            for outbox_message in &all_outbox_messages {
                if let Err(e) = outbox_batcher.submit(outbox_message.clone()).await {
                    error!("Failed to submit outbox message: {}", e);
                    outbox_success = false;
                }
            }

            let outbox_duration = outbox_start.elapsed();

            if !outbox_success {
                error!("Failed to submit some outbox messages");
                // Don't fail the entire batch for outbox errors, just log them
            }

            info!(
                "[DEBUG] execute_batch: Completed successfully - {} operations, {} outbox messages, outbox_success={}",
                batch.operations.len(),
                all_outbox_messages.len(),
                outbox_success
            );

            // Create success results for all operations
            for (operation_id, result_id, op_duration) in operation_results {
                results.push((
                    operation_id,
                    WriteOperationResult {
                        operation_id,
                        success: true,
                        result: Some(result_id),
                        error: None,
                        duration: op_duration,
                    },
                ));
            }

            println!(
                "[DEBUG] execute_batch: Finished batch with {} results",
                results.len()
            );
            return results;
        }

        // If we get here, all retries failed
        println!(
            "[DEBUG] execute_batch: All retries failed, returning {} failure results",
            batch.operations.len()
        );
        results
    }

    /// Execute a batch of operations in a single database transaction with pre-fetched versions
    /// This method prevents version race conditions by using pre-fetched current versions
    pub async fn execute_batch_with_versions(
        batch: &WriteBatch,
        event_store: &Arc<dyn EventStoreTrait>,
        projection_store: &Arc<dyn ProjectionStoreTrait>,
        write_pool: &Arc<PgPool>,
        outbox_batcher: &crate::infrastructure::cdc_debezium::OutboxBatcher,
        config: &WriteBatchingConfig,
        current_versions: &HashMap<Uuid, i64>,
    ) -> Vec<(Uuid, WriteOperationResult)> {
        println!(
            "[DEBUG] execute_batch_with_versions: Starting batch with {} operations and {} pre-fetched versions",
            batch.operations.len(),
            current_versions.len()
        );
        let mut results = Vec::new();
        let max_retries = 3;
        let mut retry_count = 0;

        while retry_count < max_retries {
            // Always start a new transaction for each retry attempt
            let mut transaction = match write_pool.begin().await {
                Ok(tx) => tx,
                Err(e) => {
                    retry_count += 1;
                    if retry_count >= max_retries {
                        for (operation_id, _) in &batch.operations {
                            results.push((
                                *operation_id,
                                WriteOperationResult {
                                    operation_id: *operation_id,
                                    success: false,
                                    result: None,
                                    error: Some(format!(
                                        "Failed to begin transaction after {} retries: {}",
                                        max_retries, e
                                    )),
                                    duration: Duration::ZERO,
                                },
                            ));
                        }
                        return results;
                    }
                    continue;
                }
            };

            let batch_start = Instant::now();

            // Collect all events and outbox messages from all operations
            let mut all_events = Vec::new();
            let mut all_outbox_messages = Vec::new();
            let mut operation_results = Vec::new();

            for (operation_id, operation) in &batch.operations {
                let op_start = Instant::now();
                let (events, outbox_messages, result_id) = match operation {
                    WriteOperation::CreateAccount {
                        account_id,
                        owner_name,
                        initial_balance,
                    } => Self::prepare_create_account_operation(
                        *account_id,
                        owner_name,
                        *initial_balance,
                    ),
                    WriteOperation::DepositMoney { account_id, amount } => {
                        Self::prepare_deposit_money_operation(*account_id, *amount)
                    }
                    WriteOperation::WithdrawMoney { account_id, amount } => {
                        Self::prepare_withdraw_money_operation(*account_id, *amount)
                    }
                };

                all_events.extend(events);
                all_outbox_messages.extend(outbox_messages);

                operation_results.push((*operation_id, result_id, op_start.elapsed()));
            }

            info!(
                "[DEBUG] execute_batch_with_versions: Prepared {} events and {} outbox messages",
                all_events.len(),
                all_outbox_messages.len()
            );

            // Group events by aggregate_id using pre-fetched versions
            let mut events_by_aggregate: Vec<(Uuid, Vec<AccountEvent>, i64)> = Vec::new();

            for event in all_events {
                let aggregate_id = match &event {
                    AccountEvent::AccountCreated { account_id, .. } => *account_id,
                    AccountEvent::MoneyDeposited { account_id, .. } => *account_id,
                    AccountEvent::MoneyWithdrawn { account_id, .. } => *account_id,
                    AccountEvent::AccountClosed { account_id, .. } => *account_id,
                };

                // Find or create entry for this aggregate
                let entry = events_by_aggregate
                    .iter_mut()
                    .find(|(id, _, _)| *id == aggregate_id);

                match entry {
                    Some((_, events, expected_version)) => {
                        // For subsequent events on the same aggregate, increment the expected version
                        *expected_version += 1;
                        events.push(event);
                    }
                    None => {
                        // Use pre-fetched version instead of calling get_current_version
                        // For new accounts, use version 0
                        let is_new_aggregate = match &event {
                            AccountEvent::AccountCreated { .. } => true,
                            _ => false,
                        };
                        let current_version = if is_new_aggregate {
                            0 // New accounts start at version 0
                        } else {
                            current_versions.get(&aggregate_id).copied().unwrap_or(0)
                        };
                        info!(
                            "[DEBUG] Using pre-fetched version {} for aggregate {}",
                            current_version, aggregate_id
                        );
                        // First event uses current_version, subsequent events will increment
                        events_by_aggregate.push((aggregate_id, vec![event], current_version));
                    }
                }
            }

            println!("[DEBUG] execute_batch_with_versions: About to call save_events_multi_aggregate_in_transaction with {} aggregates", events_by_aggregate.len());
            let result = event_store
                .save_events_multi_aggregate_in_transaction(&mut transaction, events_by_aggregate)
                .await;
            println!(
                "[DEBUG] execute_batch_with_versions: Transaction complete, results: {:?}",
                result
            );

            if let Err(e) = result {
                println!("[DEBUG] execute_batch_with_versions: Failed to save events in multi-aggregate batch: {:?}", e);
                error!("Failed to save events in multi-aggregate batch: {:?}", e);

                // Check if this is a serialization conflict that we should retry
                let is_serialization_conflict = e
                    .to_string()
                    .contains("could not serialize access due to read/write dependencies");

                if is_serialization_conflict && retry_count < max_retries - 1 {
                    // Rollback transaction and retry with exponential backoff
                    let _ = transaction.rollback().await;
                    retry_count += 1;
                    let backoff = Duration::from_millis(50 * retry_count as u64); // Use fixed backoff
                    info!(
                        "[DEBUG] Serialization conflict detected, retrying in {:?} (attempt {}/{})",
                        backoff, retry_count, max_retries
                    );
                    tokio::time::sleep(backoff).await;
                    continue;
                } else {
                    // Rollback transaction on any error
                    let _ = transaction.rollback().await;
                    retry_count += 1;
                    if retry_count >= max_retries {
                        // Return failure results for all operations
                        for (operation_id, _, _) in operation_results {
                            results.push((
                                operation_id,
                                WriteOperationResult {
                                    operation_id,
                                    success: false,
                                    result: None,
                                    error: Some(format!(
                                        "Failed to insert events after {} retries: {}",
                                        max_retries, e
                                    )),
                                    duration: Duration::ZERO,
                                },
                            ));
                        }
                        return results;
                    }
                    continue;
                }
            }

            // Commit transaction FIRST
            if let Err(e) = transaction.commit().await {
                retry_count += 1;
                if retry_count >= max_retries {
                    for (operation_id, _, _) in operation_results {
                        results.push((
                            operation_id,
                            WriteOperationResult {
                                operation_id,
                                success: false,
                                result: None,
                                error: Some(format!(
                                    "Failed to commit transaction after {} retries: {}",
                                    max_retries, e
                                )),
                                duration: Duration::ZERO,
                            },
                        ));
                    }
                    return results;
                }
                continue;
            }

            // Transaction committed successfully, now process outbox messages
            let outbox_start = Instant::now();
            let mut outbox_success = true;

            // Submit all outbox messages
            for outbox_message in &all_outbox_messages {
                if let Err(e) = outbox_batcher.submit(outbox_message.clone()).await {
                    error!("Failed to submit outbox message: {}", e);
                    outbox_success = false;
                }
            }

            let outbox_duration = outbox_start.elapsed();

            if !outbox_success {
                error!("Failed to submit some outbox messages");
                // Don't fail the entire batch for outbox errors, just log them
            }

            info!(
                "[DEBUG] execute_batch_with_versions: Completed successfully - {} operations, {} outbox messages, outbox_success={}",
                batch.operations.len(),
                all_outbox_messages.len(),
                outbox_success
            );

            // Create success results for all operations
            for (operation_id, result_id, op_duration) in operation_results {
                results.push((
                    operation_id,
                    WriteOperationResult {
                        operation_id,
                        success: true,
                        result: Some(result_id),
                        error: None,
                        duration: op_duration,
                    },
                ));
            }

            println!(
                "[DEBUG] execute_batch_with_versions: Finished batch with {} results",
                results.len()
            );
            return results;
        }

        // If we get here, all retries failed
        println!(
            "[DEBUG] execute_batch_with_versions: All retries failed, returning {} failure results",
            batch.operations.len()
        );
        results
    }

    // Helper methods to prepare operations without executing them
    fn prepare_create_account_operation(
        account_id: Uuid,
        owner_name: &str,
        initial_balance: Decimal,
    ) -> (Vec<AccountEvent>, Vec<OutboxMessage>, Uuid) {
        let event = AccountEvent::AccountCreated {
            account_id,
            owner_name: owner_name.to_string(),
            initial_balance,
        };

        let outbox_message = OutboxMessage {
            aggregate_id: account_id,
            event_id: Uuid::new_v4(),
            event_type: "AccountCreated".to_string(),
            payload: bincode::serialize(&event).unwrap_or_default(),
            topic: "banking-es.public.kafka_outbox_cdc".to_string(), // FIXED: Use correct CDC topic format
            metadata: None,
        };

        (vec![event], vec![outbox_message], account_id)
    }

    fn prepare_deposit_money_operation(
        account_id: Uuid,
        amount: Decimal,
    ) -> (Vec<AccountEvent>, Vec<OutboxMessage>, Uuid) {
        let event = AccountEvent::MoneyDeposited {
            account_id,
            amount,
            transaction_id: Uuid::new_v4(),
        };

        let outbox_message = OutboxMessage {
            aggregate_id: account_id,
            event_id: Uuid::new_v4(),
            event_type: "MoneyDeposited".to_string(),
            payload: bincode::serialize(&event).unwrap_or_default(),
            topic: "banking-es.public.kafka_outbox_cdc".to_string(), // FIXED: Use correct CDC topic format
            metadata: None,
        };

        (vec![event], vec![outbox_message], account_id)
    }

    fn prepare_withdraw_money_operation(
        account_id: Uuid,
        amount: Decimal,
    ) -> (Vec<AccountEvent>, Vec<OutboxMessage>, Uuid) {
        let event = AccountEvent::MoneyWithdrawn {
            account_id,
            amount,
            transaction_id: Uuid::new_v4(),
        };

        let outbox_message = OutboxMessage {
            aggregate_id: account_id,
            event_id: Uuid::new_v4(),
            event_type: "MoneyWithdrawn".to_string(),
            payload: bincode::serialize(&event).unwrap_or_default(),
            topic: "banking-es.public.kafka_outbox_cdc".to_string(), // FIXED: Use correct CDC topic format
            metadata: None,
        };

        (vec![event], vec![outbox_message], account_id)
    }

    /// Execute create account operation
    async fn execute_create_account(
        transaction: &mut Transaction<'_, sqlx::Postgres>,
        event_store: &Arc<dyn EventStoreTrait>,
        projection_store: &Arc<dyn ProjectionStoreTrait>,
        outbox_batcher: &crate::infrastructure::cdc_debezium::OutboxBatcher,
        account_id: Uuid,
        owner_name: &str,
        initial_balance: Decimal,
    ) -> Result<Uuid> {
        let account = Account::new(account_id, owner_name.to_string(), initial_balance);

        let event = AccountEvent::AccountCreated {
            account_id,
            owner_name: owner_name.to_string(),
            initial_balance,
        };

        // Save event to event store
        info!(
            "üîß [WriteBatching] Saving MoneyWithdrawn event to event store for account {}",
            account_id
        );
        event_store
            .save_events_in_transaction(transaction, account_id, vec![event.clone()], 0)
            .await
            .map_err(|e| {
                error!("üîß [WriteBatching] Event store error for withdraw: {:?}", e);
                anyhow::anyhow!("Event store error: {:?}", e)
            })?;
        info!("üîß [WriteBatching] Successfully saved MoneyWithdrawn event to event store for account {}", account_id);

        // Create outbox message for CDC pipeline
        let outbox_message = OutboxMessage {
            aggregate_id: account_id,
            event_id: Uuid::new_v4(),
            event_type: "AccountCreated".to_string(),
            payload: bincode::serialize(&event)
                .map_err(|e| anyhow::anyhow!("Event serialization error: {:?}", e))?,
            topic: "banking-es.public.kafka_outbox_cdc".to_string(), // FIXED: Use correct CDC topic format
            metadata: None,
        };

        // Synchronous, robust outbox submission with retries
        let mut retries = 0;
        loop {
            match outbox_batcher.submit(outbox_message.clone()).await {
                Ok(_) => break,
                Err(e) => {
                    retries += 1;
                    if retries > 5 {
                        return Err(anyhow::anyhow!(
                            "Failed to submit outbox message after retries: {}",
                            e
                        ));
                    }
                    error!(
                        "Failed to submit outbox message, retrying (attempt {}/5): {}",
                        retries, e
                    );
                    tokio::time::sleep(Duration::from_millis(50)).await;
                }
            }
        }

        Ok(account_id)
    }

    /// Execute deposit money operation
    async fn execute_deposit_money(
        transaction: &mut Transaction<'_, sqlx::Postgres>,
        event_store: &Arc<dyn EventStoreTrait>,
        projection_store: &Arc<dyn ProjectionStoreTrait>,
        outbox_batcher: &crate::infrastructure::cdc_debezium::OutboxBatcher,
        account_id: Uuid,
        amount: Decimal,
    ) -> Result<Uuid> {
        let event = AccountEvent::MoneyDeposited {
            account_id,
            amount,
            transaction_id: Uuid::new_v4(),
        };

        // Get current version of the aggregate to avoid optimistic concurrency conflicts
        // For existing accounts (deposit), use is_new_aggregate: false
        let current_version = event_store
            .get_current_version(account_id, false)
            .await
            .map_err(|e| anyhow::anyhow!("Failed to get current version: {:?}", e))?;

        // Save event to event store with correct expected version
        event_store
            .save_events_in_transaction(
                transaction,
                account_id,
                vec![event.clone()],
                current_version,
            )
            .await
            .map_err(|e| anyhow::anyhow!("Event store error: {:?}", e))?;

        // Create outbox message for CDC pipeline
        let outbox_message = OutboxMessage {
            aggregate_id: account_id,
            event_id: Uuid::new_v4(),
            event_type: "MoneyDeposited".to_string(),
            payload: bincode::serialize(&event)
                .map_err(|e| anyhow::anyhow!("Event serialization error: {:?}", e))?,
            topic: "banking-es.public.kafka_outbox_cdc".to_string(), // FIXED: Use correct CDC topic format
            metadata: None,
        };

        // Synchronous, robust outbox submission with retries
        let mut retries = 0;
        loop {
            match outbox_batcher.submit(outbox_message.clone()).await {
                Ok(_) => break,
                Err(e) => {
                    retries += 1;
                    if retries > 5 {
                        return Err(anyhow::anyhow!(
                            "Failed to submit outbox message after retries: {}",
                            e
                        ));
                    }
                    error!(
                        "Failed to submit outbox message, retrying (attempt {}/5): {}",
                        retries, e
                    );
                    tokio::time::sleep(Duration::from_millis(50)).await;
                }
            }
        }

        Ok(account_id)
    }

    /// Execute withdraw money operation
    async fn execute_withdraw_money(
        transaction: &mut Transaction<'_, sqlx::Postgres>,
        event_store: &Arc<dyn EventStoreTrait>,
        projection_store: &Arc<dyn ProjectionStoreTrait>,
        outbox_batcher: &crate::infrastructure::cdc_debezium::OutboxBatcher,
        account_id: Uuid,
        amount: Decimal,
    ) -> Result<Uuid> {
        info!(
            "üîß [WriteBatching] Starting withdraw_money for account {} with amount {}",
            account_id, amount
        );
        let event = AccountEvent::MoneyWithdrawn {
            account_id,
            amount,
            transaction_id: Uuid::new_v4(),
        };

        // Get current version of the aggregate to avoid optimistic concurrency conflicts
        // For existing accounts (withdraw), use is_new_aggregate: false
        let current_version = event_store
            .get_current_version(account_id, false)
            .await
            .map_err(|e| anyhow::anyhow!("Failed to get current version: {:?}", e))?;

        // Save event to event store with correct expected version
        event_store
            .save_events_in_transaction(
                transaction,
                account_id,
                vec![event.clone()],
                current_version,
            )
            .await
            .map_err(|e| anyhow::anyhow!("Event store error: {:?}", e))?;

        // Create outbox message for CDC pipeline
        info!(
            "üîß [WriteBatching] Creating outbox message for MoneyWithdrawn event for account {}",
            account_id
        );
        let outbox_message = OutboxMessage {
            aggregate_id: account_id,
            event_id: Uuid::new_v4(),
            event_type: "MoneyWithdrawn".to_string(),
            payload: bincode::serialize(&event).map_err(|e| {
                error!(
                    "üîß [WriteBatching] Event serialization error for withdraw: {:?}",
                    e
                );
                anyhow::anyhow!("Event serialization error: {:?}", e)
            })?,
            topic: "banking-es.public.kafka_outbox_cdc".to_string(), // FIXED: Use correct CDC topic format
            metadata: None,
        };
        info!("üîß [WriteBatching] Successfully created outbox message for MoneyWithdrawn event for account {}", account_id);

        // Synchronous, robust outbox submission with retries
        let mut retries = 0;
        loop {
            match outbox_batcher.submit(outbox_message.clone()).await {
                Ok(_) => break,
                Err(e) => {
                    retries += 1;
                    if retries > 5 {
                        return Err(anyhow::anyhow!(
                            "Failed to submit outbox message after retries: {}",
                            e
                        ));
                    }
                    error!(
                        "Failed to submit outbox message, retrying (attempt {}/5): {}",
                        retries, e
                    );
                    tokio::time::sleep(Duration::from_millis(50)).await;
                }
            }
        }

        Ok(account_id)
    }

    /// Get service statistics with channel lifecycle monitoring
    pub async fn get_stats(&self) -> serde_json::Value {
        let batches_processed = *self.batches_processed.lock().await;
        let operations_processed = *self.operations_processed.lock().await;
        let pending_count = self.get_pending_operations_count().await;
        let completed_count = self.get_completed_operations_count().await;

        // Get Redis lock metrics
        let lock_metrics = self.redis_lock.get_metrics_json();

        serde_json::json!({
            "batches_processed": batches_processed,
            "operations_processed": operations_processed,
            "pending_operations": pending_count,
            "completed_operations": completed_count,
            "redis_lock_metrics": lock_metrics,
            "partition_id": self.partition_id,
            "num_partitions": self.num_partitions,
        })
    }
}

/// Helper to check if an error is a version conflict
fn is_version_conflict(e: &crate::infrastructure::event_store::EventStoreError) -> bool {
    let msg = format!("{}", e);
    msg.contains("Invalid version sequence")
        || msg.contains(
            "could not serialize access due to read/write dependencies among transactions",
        )
}

/// Robust event writing with version conflict retry
pub async fn save_events_with_retry(
    event_store: &Arc<dyn EventStoreTrait>,
    aggregate_id: Uuid,
    events: Vec<AccountEvent>,
    max_retries: usize,
) -> Result<(), crate::infrastructure::event_store::EventStoreError> {
    let mut retries = 0;
    let mut backoff = 100;

    loop {
        // Get current version on each attempt to avoid race conditions
        let current_version = event_store.get_current_version(aggregate_id, false).await?;

        let result = event_store
            .save_events(aggregate_id, events.clone(), current_version)
            .await;

        match result {
            Ok(_) => return Ok(()),
            Err(e) if is_version_conflict(&e) && retries < max_retries => {
                retries += 1;
                println!("[DEBUG] Version conflict for aggregate_id={:?}, retrying (attempt {}/{}), current_version={}", 
                    aggregate_id, retries, max_retries, current_version);
                let jitter = rand::thread_rng().gen_range(0..50);
                tokio::time::sleep(std::time::Duration::from_millis(backoff + jitter)).await;
                backoff = std::cmp::min(backoff * 2, 1000); // Cap backoff at 1 second
            }
            Err(e) => return Err(e),
        }
    }
}

#[cfg(test)]
#[ignore]
mod tests {
    use super::*;

    #[tokio::test]
    #[ignore]
    async fn test_write_batching_config() {
        let config = WriteBatchingConfig::default();
        assert_eq!(config.max_batch_size, 50);
        assert_eq!(config.max_batch_wait_time_ms, 100);
        assert_eq!(config.max_retries, 2);
    }

    #[tokio::test]
    #[ignore]
    async fn test_write_batch_creation() {
        let mut batch = WriteBatch::new();
        assert_eq!(batch.operations.len(), 0);

        batch.add_operation(
            Uuid::new_v4(),
            WriteOperation::CreateAccount {
                account_id: Uuid::new_v4(),
                owner_name: "Test User".to_string(),
                initial_balance: Decimal::new(1000, 0),
            },
        );

        assert_eq!(batch.operations.len(), 1);
    }

    #[tokio::test]
    #[ignore]
    async fn test_write_batch_should_process() {
        let mut batch = WriteBatch::new();

        // Test size-based processing
        for _ in 0..50 {
            batch.add_operation(
                Uuid::new_v4(),
                WriteOperation::CreateAccount {
                    account_id: Uuid::new_v4(),
                    owner_name: "Test User".to_string(),
                    initial_balance: Decimal::new(1000, 0),
                },
            );
        }

        assert!(batch.should_process(50, Duration::from_millis(10)));
        assert!(batch.is_full(50));
    }
}
