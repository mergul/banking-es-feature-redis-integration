use crate::domain::{Account, AccountEvent};
use crate::infrastructure::event_store::{EventStoreConfig, EventStoreTrait};
use crate::infrastructure::outbox::{OutboxMessage, OutboxRepositoryTrait};
use crate::infrastructure::projections::{ProjectionConfig, ProjectionStoreTrait};
use anyhow::{Context, Result};
use bincode;
use chrono::Utc;
use rust_decimal::Decimal;
use sqlx::{PgPool, Postgres, Transaction};
use std::collections::hash_map::DefaultHasher;
use std::collections::HashMap;
use std::hash::{Hash, Hasher};
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::{mpsc, Mutex};
use tokio_util::sync::CancellationToken;
use tracing::{debug, error, info, warn};
use uuid::Uuid;

use crate::infrastructure::redis_aggregate_lock::{
    OperationType, RedisAggregateLock, RedisLockConfig,
};
use rand::Rng;

const NUM_PARTITIONS: usize = 8; // Increased from 4 to 8 to reduce partition conflicts
const DEFAULT_BATCH_SIZE: usize = 1000;
const CREATE_BATCH_SIZE: usize = 1000; // Optimized batch size for create operations
const CREATE_PARTITION_ID: usize = 0; // Dedicated partition for create operations
const WRITE_POOL_SIZE: usize = 400; // Optimized write pool size for single pool usage
const DEFAULT_BATCH_TIMEOUT_MS: u64 = 25;

fn partition_for_aggregate(aggregate_id: &Uuid, num_partitions: usize) -> usize {
    let mut hasher = DefaultHasher::new();
    aggregate_id.hash(&mut hasher);
    (hasher.finish() as usize) % num_partitions
}

/// Partitioned batching manager
pub struct PartitionedBatching {
    processors: Vec<Arc<WriteBatchingService>>,
}

impl PartitionedBatching {
    pub async fn new(
        event_store: Arc<dyn EventStoreTrait>,
        write_pool: Arc<PgPool>,
        outbox_batcher: crate::infrastructure::cdc_debezium::OutboxBatcher,
    ) -> Self {
        let num_partitions = NUM_PARTITIONS;
        let mut processors = Vec::new();

        for partition_id in 0..num_partitions {
            let config = WriteBatchingConfig::default();
            let processor = WriteBatchingService::new_for_partition(
                partition_id,
                num_partitions,
                config,
                event_store.clone(),
                write_pool.clone(),
            );
            processors.push(Arc::new(processor));
        }

        Self { processors }
    }

    pub async fn submit_operation(&self, aggregate_id: Uuid, op: WriteOperation) -> Result<Uuid> {
        let partition = partition_for_aggregate(&aggregate_id, NUM_PARTITIONS);
        self.processors[partition]
            .submit_operation(aggregate_id, op)
            .await
    }

    /// Phase 1: Create-specific batch method for bulk account creation
    /// This method handles aggregate_id collisions by using a dedicated create partition
    /// and optimized batch processing for bulk operations
    pub async fn submit_create_operations_batch(
        &self,
        accounts: Vec<(String, Decimal)>,
    ) -> Result<Vec<Uuid>> {
        info!(
            "üöÄ Starting true bulk create operation for {} accounts",
            accounts.len()
        );

        if accounts.is_empty() {
            return Ok(Vec::new());
        }

        // Use dedicated create partition to avoid collisions with update operations
        let create_processor = &self.processors[CREATE_PARTITION_ID];

        // Create a single batch with all operations
        let mut batch = WriteBatch::new(Some(CREATE_PARTITION_ID));
        let mut account_ids = Vec::with_capacity(accounts.len());

        for (owner_name, balance) in accounts {
            let account_id = Uuid::new_v4();
            let operation = WriteOperation::CreateAccount {
                account_id,
                owner_name,
                initial_balance: balance,
            };

            // Add to the same batch
            let operation_id = Uuid::new_v4();
            batch.add_operation(operation_id, operation);
            account_ids.push(account_id);
        }

        info!(
            "üì¶ Created single batch with {} create operations",
            batch.operations.len()
        );

        // Process the entire batch at once
        let batch_start = Instant::now();

        // Execute the batch directly using WriteBatchingService unified method
        let results = WriteBatchingService::execute_batch_unified(
            &batch,
            &create_processor.event_store,
            &create_processor.write_pool,
            &create_processor.config,
            batch.partition_id,
        )
        .await;

        let batch_duration = batch_start.elapsed();
        info!(
            "‚úÖ True batch processing completed in {:?} for {} operations",
            batch_duration,
            batch.operations.len()
        );

        // Extract successful account IDs from results
        let mut successful_account_ids = Vec::new();
        for (operation_id, result) in results {
            if result.success {
                if let Some(account_id) = result.result {
                    successful_account_ids.push(account_id);
                    info!("‚úÖ Account created successfully: {}", account_id);
                }
            } else {
                error!(
                    "‚ùå Account creation failed for operation {}: {}",
                    operation_id,
                    result.error.unwrap_or_else(|| "Unknown error".to_string())
                );
            }
        }

        info!(
            "‚úÖ True bulk create operation completed: {} successful, {} failed",
            successful_account_ids.len(),
            batch.operations.len() - successful_account_ids.len()
        );

        Ok(successful_account_ids)
    }

    /// Alternative method for create operations that uses collision-aware partitioning
    /// This method distributes create operations across partitions while handling collisions
    pub async fn submit_create_operations_distributed(
        &self,
        accounts: Vec<(String, Decimal)>,
    ) -> Result<Vec<Uuid>> {
        info!(
            "üöÄ Starting distributed create operation for {} accounts",
            accounts.len()
        );

        let mut operation_ids = Vec::with_capacity(accounts.len());
        let mut account_to_partition = HashMap::new();

        // Distribute create operations across partitions to avoid overwhelming single partition
        for (owner_name, balance) in accounts {
            let account_id = Uuid::new_v4();
            let operation = WriteOperation::CreateAccount {
                account_id,
                owner_name,
                initial_balance: balance,
            };

            // Use hash-based partitioning but with collision detection
            let partition = partition_for_aggregate(&account_id, NUM_PARTITIONS);
            account_to_partition.insert(account_id, partition);

            let op_id = self.processors[partition]
                .submit_operation(account_id, operation)
                .await?;
            operation_ids.push((op_id, account_id, partition));
        }

        info!(
            "üì¶ Distributed {} create operations across {} partitions",
            operation_ids.len(),
            NUM_PARTITIONS
        );

        // Wait for all operations to complete
        let mut results = Vec::with_capacity(operation_ids.len());
        let mut failed_operations = Vec::new();

        for (op_id, account_id, partition) in operation_ids {
            match self.processors[partition].wait_for_result(op_id).await {
                Ok(result) => {
                    if result.success {
                        if let Some(created_account_id) = result.result {
                            results.push(created_account_id);
                            info!(
                                "‚úÖ Account created successfully on partition {}: {}",
                                partition, created_account_id
                            );
                        } else {
                            failed_operations
                                .push((account_id, "No account ID returned".to_string()));
                        }
                    } else {
                        failed_operations.push((
                            account_id,
                            result.error.unwrap_or_else(|| "Unknown error".to_string()),
                        ));
                    }
                }
                Err(e) => {
                    failed_operations.push((account_id, format!("{}", e)));
                }
            }
        }

        info!(
            "‚úÖ Distributed create operation completed: {} successful, {} failed",
            results.len(),
            failed_operations.len()
        );

        if !failed_operations.is_empty() {
            warn!(
                "‚ö†Ô∏è Some distributed create operations failed: {:?}",
                failed_operations
            );
        }

        Ok(results)
    }

    /// Phase 2: True batch processing for write operations (deposits/withdrawals)
    /// This method handles aggregate_id and version collisions by using true batch processing
    /// with proper partitioning to avoid conflicts between different aggregates
    pub async fn submit_write_operations_batch(
        &self,
        operations: Vec<(Uuid, Vec<AccountEvent>)>, // (aggregate_id, events)
    ) -> Result<Vec<Uuid>> {
        if operations.is_empty() {
            return Ok(Vec::new());
        }

        info!(
            "üöÄ Starting bulk write operations batch with {} aggregates",
            operations.len()
        );

        let start_time = Instant::now();
        let mut all_operation_ids = Vec::new();

        // Process each aggregate's events in parallel
        let mut handles = Vec::new();
        for (aggregate_id, events) in operations {
            let processor = Arc::clone(&self.processors[0]); // Use first processor for bulk operations
            let handle = tokio::spawn(async move {
                let operation = WriteOperation::DepositMoney {
                    account_id: aggregate_id,
                    amount: Decimal::new(0, 0), // Placeholder - actual events will be used
                };
                processor.submit_operation(aggregate_id, operation).await
            });
            handles.push(handle);
        }

        // Wait for all operations to complete
        for handle in handles {
            match handle.await {
                Ok(Ok(operation_id)) => all_operation_ids.push(operation_id),
                Ok(Err(e)) => {
                    error!("Bulk operation failed: {}", e);
                    return Err(e);
                }
                Err(e) => {
                    error!("Bulk operation task failed: {}", e);
                    return Err(anyhow::anyhow!("Task join error: {}", e));
                }
            }
        }

        let duration = start_time.elapsed();
        info!(
            "‚úÖ Bulk write operations completed: {} operations in {:?}",
            all_operation_ids.len(),
            duration
        );

        Ok(all_operation_ids)
    }

    /// High-performance bulk submit method for maximum throughput
    /// This method processes operations in large batches for optimal performance
    pub async fn submit_operations_bulk_optimized(
        &self,
        operations: Vec<WriteOperation>,
        batch_size: usize,
    ) -> Result<Vec<Uuid>> {
        if operations.is_empty() {
            return Ok(Vec::new());
        }

        info!(
            "üöÄ Starting bulk optimized processing for {} operations with batch size {}",
            operations.len(),
            batch_size
        );

        let start_time = Instant::now();
        let mut all_operation_ids = Vec::new();

        // Split operations into batches
        for chunk in operations.chunks(batch_size) {
            let chunk_operations = chunk.to_vec();

            // Use the batch operations method for optimal distribution
            let operation_ids = self.processors[0]
                .submit_batch_operations(chunk_operations)
                .await?;

            all_operation_ids.extend(operation_ids);
        }

        let duration = start_time.elapsed();
        info!(
            "‚úÖ Bulk optimized processing completed: {} operations in {:?}",
            all_operation_ids.len(),
            duration
        );

        Ok(all_operation_ids)
    }

    /// Helper method to create WriteOperation from AccountEvents
    fn create_write_operation_from_events(
        &self,
        aggregate_id: Uuid,
        events: Vec<AccountEvent>,
    ) -> Result<WriteOperation> {
        // FIXED: For bulk processing, we want to preserve individual events
        // instead of aggregating them. The events should be processed individually
        // to maintain proper event sourcing and CDC processing.

        // For bulk operations, we need to handle multiple events per aggregate
        // The current approach of aggregating events loses individual transaction history
        // which breaks event sourcing principles.

        // Since we can't return multiple operations from this function,
        // we need to modify the bulk processing approach to handle events individually.

        // For now, let's process the first event and log a warning about aggregation
        if events.len() > 1 {
            warn!(
                "‚ö†Ô∏è  Bulk processing detected {} events for aggregate {}. Processing first event only. Consider using individual event processing for proper event sourcing.",
                events.len(),
                aggregate_id
            );
        }

        // Process the first event to maintain compatibility
        if let Some(first_event) = events.first() {
            match first_event {
                AccountEvent::MoneyDeposited { amount, .. } => Ok(WriteOperation::DepositMoney {
                    account_id: aggregate_id,
                    amount: *amount,
                }),
                AccountEvent::MoneyWithdrawn { amount, .. } => Ok(WriteOperation::WithdrawMoney {
                    account_id: aggregate_id,
                    amount: *amount,
                }),
                AccountEvent::AccountCreated { .. } => {
                    // For account creation, we need to extract the data
                    if let AccountEvent::AccountCreated {
                        account_id,
                        owner_name,
                        initial_balance,
                    } = first_event
                    {
                        Ok(WriteOperation::CreateAccount {
                            account_id: *account_id,
                            owner_name: owner_name.clone(),
                            initial_balance: *initial_balance,
                        })
                    } else {
                        Err(anyhow::anyhow!("Invalid AccountCreated event structure"))
                    }
                }
                AccountEvent::AccountClosed { .. } => {
                    // For account closure, we'll use a minimal deposit as placeholder
                    // since we don't have a CloseAccount operation type
                    warn!("‚ö†Ô∏è  AccountClosed event not supported in bulk processing. Using placeholder operation.");
                    Ok(WriteOperation::DepositMoney {
                        account_id: aggregate_id,
                        amount: Decimal::ZERO,
                    })
                }
            }
        } else {
            // No events provided - return a minimal operation
            Ok(WriteOperation::DepositMoney {
                account_id: aggregate_id,
                amount: Decimal::ZERO,
            })
        }
    }

    pub fn start_all(&self) {
        for processor in &self.processors {
            let processor = processor.clone();
            tokio::spawn(async move {
                let _ = processor.start().await;
            });
        }
    }

    /// Bulk insert modunu t√ºm partition'larda ba≈ülat
    pub async fn start_bulk_mode_all_partitions(&self) -> Result<()> {
        info!("üöÄ T√ºm partition'larda bulk insert modu ba≈ülatƒ±lƒ±yor...");

        for (partition_id, processor) in self.processors.iter().enumerate() {
            if let Err(e) = processor.start_bulk_mode().await {
                error!(
                    "Partition {} i√ßin bulk mod ba≈ülatƒ±lamadƒ±: {}",
                    partition_id, e
                );
                return Err(e);
            }
        }

        info!("‚úÖ T√ºm partition'larda bulk insert modu aktif");
        Ok(())
    }

    /// Bulk insert modunu t√ºm partition'larda sonlandƒ±r
    pub async fn end_bulk_mode_all_partitions(&self) -> Result<()> {
        info!("üîÑ T√ºm partition'larda bulk insert modu sonlandƒ±rƒ±lƒ±yor...");

        for (partition_id, processor) in self.processors.iter().enumerate() {
            if let Err(e) = processor.end_bulk_mode().await {
                error!(
                    "Partition {} i√ßin bulk mod sonlandƒ±rƒ±lamadƒ±: {}",
                    partition_id, e
                );
                return Err(e);
            }
        }

        info!("‚úÖ T√ºm partition'larda bulk insert modu sonlandƒ±rƒ±ldƒ±");
        Ok(())
    }

    /// Bulk insert ile create operations batch i≈ülemi
    pub async fn submit_create_operations_batch_with_bulk_config(
        &self,
        accounts: Vec<(String, Decimal)>,
    ) -> Result<Vec<Uuid>> {
        info!(
            "üöÄ Bulk config ile {} hesap olu≈üturma i≈ülemi ba≈ülatƒ±lƒ±yor",
            accounts.len()
        );

        // Bulk modu ba≈ülat
        self.start_bulk_mode_all_partitions().await?;

        let results = self.submit_create_operations_batch(accounts).await?;

        // Bulk modu sonlandƒ±r
        self.end_bulk_mode_all_partitions().await?;

        info!(
            "‚úÖ Bulk config ile hesap olu≈üturma i≈ülemi tamamlandƒ±: {} ba≈üarƒ±lƒ±",
            results.len()
        );
        Ok(results)
    }

    /// Bulk insert ile write operations batch i≈ülemi
    pub async fn submit_write_operations_batch_with_bulk_config(
        &self,
        operations: Vec<(Uuid, Vec<AccountEvent>)>,
    ) -> Result<Vec<Uuid>> {
        info!(
            "üöÄ Bulk config ile {} write operation batch i≈ülemi ba≈ülatƒ±lƒ±yor",
            operations.len()
        );

        // Bulk modu ba≈ülat
        self.start_bulk_mode_all_partitions().await?;

        let results = self.submit_write_operations_batch(operations).await?;

        // Bulk modu sonlandƒ±r
        self.end_bulk_mode_all_partitions().await?;

        info!(
            "‚úÖ Bulk config ile write operations batch i≈ülemi tamamlandƒ±: {} ba≈üarƒ±lƒ±",
            results.len()
        );
        Ok(results)
    }

    /// Bulk insert ile operations submit i≈ülemi
    pub async fn submit_operations_with_bulk_config(
        &self,
        operations: Vec<WriteOperation>,
    ) -> Result<Vec<Uuid>> {
        info!(
            "üöÄ Bulk config ile {} operation submit i≈ülemi ba≈ülatƒ±lƒ±yor",
            operations.len()
        );

        // Bulk modu ba≈ülat
        self.start_bulk_mode_all_partitions().await?;

        let results = self.submit_operations_as_direct_batches(operations).await?;

        // Bulk modu sonlandƒ±r
        self.end_bulk_mode_all_partitions().await?;

        info!(
            "‚úÖ Bulk config ile operations submit i≈ülemi tamamlandƒ±: {} ba≈üarƒ±lƒ±",
            results.len()
        );
        Ok(results)
    }

    /// Wait for the result of a submitted operation by searching all partitions
    pub async fn wait_for_result(
        &self,
        operation_id: Uuid,
    ) -> Result<WriteOperationResult, anyhow::Error> {
        // Try each partition in parallel (or sequentially, since there are few)
        for processor in &self.processors {
            // Try to get the result; if not found, continue
            match processor.wait_for_result(operation_id).await {
                Ok(result) => return Ok(result),
                Err(e) => {
                    // If the error is not 'not found', return it
                    let msg = format!("{}", e);
                    if !msg.contains("not found") && !msg.contains("already completed") {
                        return Err(e);
                    }
                }
            }
        }
        Err(anyhow::anyhow!(
            "Operation {} not found in any partition",
            operation_id
        ))
    }

    /// Get the current batch size across all partitions (for smart consistency management)
    pub async fn get_current_batch_size(&self) -> usize {
        let mut total_size = 0;
        for processor in &self.processors {
            total_size += processor.get_current_batch_size().await;
        }
        total_size
    }

    /// Aggregate-based parallel batching - groups operations by aggregate_id
    /// and processes different aggregates in parallel while keeping same aggregate_id
    /// operations in the same batch to prevent serialization conflicts
    pub async fn submit_operations_aggregate_based_parallel(
        &self,
        operations: Vec<WriteOperation>,
    ) -> Result<Vec<Uuid>> {
        if operations.is_empty() {
            return Ok(Vec::new());
        }

        info!(
            "üöÄ Starting aggregate-based parallel batching for {} operations",
            operations.len()
        );

        // Group operations by aggregate_id
        let mut aggregate_groups: HashMap<Uuid, Vec<WriteOperation>> = HashMap::new();
        let operations_len = operations.len();
        for operation in operations {
            let aggregate_id = operation.get_aggregate_id();
            aggregate_groups
                .entry(aggregate_id)
                .or_default()
                .push(operation);
        }

        let aggregate_groups_len = aggregate_groups.len();
        info!(
            "üì¶ Grouped {} operations into {} aggregate groups",
            operations_len, aggregate_groups_len
        );

        // Process each aggregate group in parallel
        let mut futures = Vec::new();
        let mut aggregate_to_operation_ids = HashMap::new();

        for (aggregate_id, aggregate_operations) in aggregate_groups {
            let processor =
                &self.processors[partition_for_aggregate(&aggregate_id, NUM_PARTITIONS)];

            // Create operation IDs for this aggregate's operations
            let operation_ids: Vec<Uuid> = aggregate_operations
                .iter()
                .map(|_| Uuid::new_v4())
                .collect();

            aggregate_to_operation_ids.insert(aggregate_id, operation_ids.clone());

            // Submit all operations for this aggregate to the same processor
            let future = async move {
                let mut results = Vec::new();
                for (operation, operation_id) in aggregate_operations.into_iter().zip(operation_ids)
                {
                    match processor
                        .submit_operation(operation.get_aggregate_id(), operation)
                        .await
                    {
                        Ok(id) => results.push(id),
                        Err(e) => {
                            error!(
                                "Failed to submit operation {} for aggregate {}: {}",
                                operation_id, aggregate_id, e
                            );
                        }
                    }
                }
                (aggregate_id, results)
            };

            futures.push(future);
        }

        // Wait for all parallel processing to complete
        let results = futures::future::join_all(futures).await;

        let mut all_operation_ids = Vec::new();
        let mut successful_count = 0;
        let mut failed_count = 0;

        for (aggregate_id, operation_ids) in results {
            all_operation_ids.extend(operation_ids.clone());
            successful_count += operation_ids.len();
            info!(
                "‚úÖ Processed {} operations for aggregate {}",
                operation_ids.len(),
                aggregate_id
            );
        }

        info!(
            "‚úÖ Aggregate-based parallel batching completed: {} successful operations across {} aggregates",
            successful_count,
            aggregate_groups_len
        );

        Ok(all_operation_ids)
    }

    /// Advanced aggregate-based batching with bulk processing
    /// Creates dedicated batches for each aggregate_id and processes them in parallel
    /// This prevents serialization conflicts by ensuring same aggregate_id operations
    /// are processed together in a single transaction
    pub async fn submit_operations_aggregate_bulk_parallel(
        &self,
        operations: Vec<WriteOperation>,
    ) -> Result<Vec<Uuid>> {
        if operations.is_empty() {
            return Ok(Vec::new());
        }

        info!(
            "üöÄ Starting aggregate-based bulk parallel batching for {} operations",
            operations.len()
        );

        // Group operations by aggregate_id
        let mut aggregate_groups: HashMap<Uuid, Vec<WriteOperation>> = HashMap::new();
        let operations_len = operations.len();
        for operation in operations {
            let aggregate_id = operation.get_aggregate_id();
            aggregate_groups
                .entry(aggregate_id)
                .or_default()
                .push(operation);
        }

        let aggregate_groups_len = aggregate_groups.len();
        info!(
            "üì¶ Grouped {} operations into {} aggregate groups",
            operations_len, aggregate_groups_len
        );

        // Create dedicated batches for each aggregate and process in parallel
        let mut futures = Vec::new();

        for (aggregate_id, aggregate_operations) in aggregate_groups {
            let processor =
                &self.processors[partition_for_aggregate(&aggregate_id, NUM_PARTITIONS)];

            // Create a dedicated batch for this aggregate
            let mut dedicated_batch = WriteBatch::new(Some(processor.partition_id.unwrap()));
            let mut operation_ids = Vec::new();

            for operation in aggregate_operations {
                let operation_id = Uuid::new_v4();
                dedicated_batch.add_operation(operation_id, operation);
                operation_ids.push(operation_id);
            }

            info!(
                "üì¶ Created dedicated batch for aggregate {} with {} operations",
                aggregate_id,
                dedicated_batch.operations.len()
            );

            // Process this aggregate's batch in parallel
            let future = async move {
                let batch_start = Instant::now();

                // Execute the dedicated batch with unified transaction
                let results = WriteBatchingService::execute_batch_unified(
                    &dedicated_batch,
                    &processor.event_store,
                    &processor.write_pool,
                    &processor.config,
                    dedicated_batch.partition_id,
                )
                .await;

                let batch_duration = batch_start.elapsed();
                let successful_count = results.iter().filter(|(_, r)| r.success).count();

                info!(
                    "‚úÖ Aggregate {} batch completed in {:?}: {}/{} successful",
                    aggregate_id,
                    batch_duration,
                    successful_count,
                    results.len()
                );

                (aggregate_id, operation_ids, results)
            };

            futures.push(future);
        }

        // Wait for all parallel batch processing to complete
        let results = futures::future::join_all(futures).await;

        let mut all_operation_ids = Vec::new();
        let mut total_successful = 0;
        let mut total_failed = 0;

        for (aggregate_id, operation_ids, batch_results) in results {
            all_operation_ids.extend(operation_ids);

            let successful = batch_results.iter().filter(|(_, r)| r.success).count();
            let failed = batch_results.len() - successful;

            total_successful += successful;
            total_failed += failed;

            if failed > 0 {
                warn!(
                    "‚ö†Ô∏è Aggregate {} had {} failed operations",
                    aggregate_id, failed
                );
            }
        }

        info!(
            "‚úÖ Aggregate-based bulk parallel batching completed: {} successful, {} failed across {} aggregates",
            total_successful,
            total_failed,
            aggregate_groups_len
        );

        Ok(all_operation_ids)
    }

    /// Hash-based super batch processing with unique aggregate_id guarantee
    /// Each aggregate_id can only appear in one super batch
    pub async fn submit_operations_hash_super_batch(
        self: Arc<Self>,
        operations: Vec<WriteOperation>,
        num_super_batches: usize,
    ) -> Result<Vec<Uuid>> {
        if operations.is_empty() {
            return Ok(Vec::new());
        }

        info!(
            "üöÄ Starting hash-based super batch processing for {} operations across {} partitions",
            operations.len(),
            num_super_batches
        );

        // Partition operations by hash
        let mut super_batches: Vec<Vec<(WriteOperation, usize)>> =
            vec![Vec::new(); num_super_batches];
        for op in operations {
            let partition = partition_for_aggregate(&op.get_aggregate_id(), num_super_batches);
            super_batches[partition].push((op, partition));
        }

        // Her partition'ƒ± paralel i≈üleyelim - FIXED: Batch processing
        let mut handles = Vec::new();
        for batch in super_batches {
            let batching_service = Arc::clone(&self);
            handles.push(tokio::spawn(async move {
                let mut ids = Vec::new();

                // FIXED: Group operations by partition and submit as batches
                let mut operations_by_partition: HashMap<usize, Vec<WriteOperation>> =
                    HashMap::new();
                for (op, partition) in batch {
                    operations_by_partition
                        .entry(partition)
                        .or_insert_with(Vec::new)
                        .push(op);
                }

                // Submit each partition's operations as a batch
                for (partition, partition_ops) in operations_by_partition {
                    let processor = &batching_service.processors[partition];

                    // Create a dedicated batch for this partition
                    let mut dedicated_batch = WriteBatch::new(Some(partition));
                    let mut operation_ids = Vec::new();

                    for operation in partition_ops {
                        let operation_id = Uuid::new_v4();
                        dedicated_batch.add_operation(operation_id, operation);
                        operation_ids.push(operation_id);
                    }

                    info!(
                        "üì¶ Created dedicated batch for partition {} with {} operations",
                        partition,
                        dedicated_batch.operations.len()
                    );

                    // Execute the dedicated batch with unified transaction
                    let batch_results = WriteBatchingService::execute_batch_unified(
                        &dedicated_batch,
                        &processor.event_store,
                        &processor.write_pool,
                        &processor.config,
                        dedicated_batch.partition_id,
                    )
                    .await;

                    // Store completed results and collect operation IDs
                    for (operation_id, result) in batch_results {
                        processor
                            .completed_results
                            .lock()
                            .await
                            .insert(operation_id, result.clone());

                        if let Some(sender) =
                            processor.pending_results.lock().await.remove(&operation_id)
                        {
                            let _ = sender.send(result).await;
                        }

                        ids.push(operation_id);
                    }
                }

                ids
            }));
        }
        let mut all_ids = Vec::new();
        for h in handles {
            if let Ok(ids) = h.await {
                all_ids.extend(ids);
            }
        }
        Ok(all_ids)
    }

    /// Alternative implementation of hash-based super batch processing using new helper methods
    /// This method provides a cleaner approach by leveraging the new batch processing helpers
    /// and offers better error handling and monitoring capabilities
    pub async fn submit_operations_hash_super_batch_v2(
        &self,
        operations: Vec<WriteOperation>,
        num_super_batches: usize,
        use_locking: bool,
    ) -> Result<Vec<Uuid>> {
        if operations.is_empty() {
            return Ok(Vec::new());
        }

        let operations_len = operations.len();
        info!(
            "üöÄ Starting hash-based super batch processing v2 for {} operations across {} partitions (locking: {})",
            operations_len,
            num_super_batches,
            use_locking
        );

        // Partition operations by hash using the same logic as original
        let mut super_batches: Vec<Vec<WriteOperation>> = vec![Vec::new(); num_super_batches];
        for operation in operations {
            let partition =
                partition_for_aggregate(&operation.get_aggregate_id(), num_super_batches);
            super_batches[partition].push(operation);
        }

        // Count non-empty batches for logging
        let non_empty_batches = super_batches
            .iter()
            .filter(|batch| !batch.is_empty())
            .count();
        info!(
            "üì¶ Distributed {} operations into {} non-empty super batches",
            operations_len, non_empty_batches
        );

        // Process each super batch using the appropriate helper method
        let mut futures = Vec::new();
        for (batch_index, batch_operations) in super_batches.into_iter().enumerate() {
            if batch_operations.is_empty() {
                continue; // Skip empty batches
            }

            let batching_service = self.clone();
            let use_locking = use_locking;

            let future = async move {
                info!(
                    "üîß Processing super batch {} with {} operations (locking: {})",
                    batch_index,
                    batch_operations.len(),
                    use_locking
                );

                // Use the appropriate helper method based on locking preference
                let result = if use_locking {
                    batching_service
                        .submit_operations_as_locked_batches(batch_operations)
                        .await
                } else {
                    batching_service
                        .submit_operations_as_direct_batches(batch_operations)
                        .await
                };

                match result {
                    Ok(operation_ids) => {
                        info!(
                            "‚úÖ Super batch {} completed successfully with {} operations",
                            batch_index,
                            operation_ids.len()
                        );
                        Ok(operation_ids)
                    }
                    Err(e) => {
                        error!("‚ùå Super batch {} failed: {}", batch_index, e);
                        Err(e)
                    }
                }
            };

            futures.push(future);
        }

        // Wait for all super batches to complete
        let batch_start = Instant::now();
        let results = futures::future::join_all(futures).await;
        let batch_duration = batch_start.elapsed();

        // Collect results and handle errors
        let mut all_operation_ids = Vec::new();
        let mut successful_batches = 0;
        let mut failed_batches = 0;
        let mut total_successful_operations = 0;
        let mut total_failed_operations = 0;

        for (batch_index, result) in results.into_iter().enumerate() {
            match result {
                Ok(operation_ids) => {
                    let operation_count = operation_ids.len();
                    all_operation_ids.extend(operation_ids);
                    successful_batches += 1;
                    total_successful_operations += operation_count;

                    info!(
                        "‚úÖ Super batch {}: {} operations processed successfully",
                        batch_index, operation_count
                    );
                }
                Err(e) => {
                    failed_batches += 1;
                    total_failed_operations += 1; // Count as 1 failed operation for the batch

                    error!("‚ùå Super batch {}: failed with error: {}", batch_index, e);
                }
            }
        }

        info!(
            "üéØ Hash-based super batch processing v2 completed in {:?}: {} successful batches ({} operations), {} failed batches",
            batch_duration,
            successful_batches,
            total_successful_operations,
            failed_batches
        );

        // Return all operation IDs (both successful and failed)
        Ok(all_operation_ids)
    }

    /// Enhanced version with adaptive batch sizing and performance monitoring
    /// This method automatically adjusts batch sizes based on operation types and system load
    pub async fn submit_operations_adaptive_super_batch(
        &self,
        operations: Vec<WriteOperation>,
        num_super_batches: usize,
        adaptive_config: AdaptiveBatchConfig,
    ) -> Result<Vec<Uuid>> {
        if operations.is_empty() {
            return Ok(Vec::new());
        }

        let operations_len = operations.len();
        info!(
            "üöÄ Starting adaptive super batch processing for {} operations across {} partitions",
            operations_len, num_super_batches
        );

        // Analyze operation types to determine optimal processing strategy
        let operation_analysis = self.analyze_operations(&operations);
        info!(
            "üìä Operation analysis: {} creates, {} deposits, {} withdrawals",
            operation_analysis.create_count,
            operation_analysis.deposit_count,
            operation_analysis.withdrawal_count
        );

        // Determine processing strategy based on operation mix
        let use_locking = self.should_use_locking(&operation_analysis, &adaptive_config);
        let batch_size_limit =
            self.calculate_optimal_batch_size(&operation_analysis, &adaptive_config);

        info!(
            "‚öôÔ∏è Adaptive configuration: locking={}, batch_size_limit={}",
            use_locking, batch_size_limit
        );

        // Split operations into optimal batches
        let optimized_batches = self.split_operations_optimally(
            operations,
            num_super_batches,
            batch_size_limit,
            &operation_analysis,
        );

        info!(
            "üì¶ Created {} optimized batches with average size {}",
            optimized_batches.len(),
            operations_len / optimized_batches.len().max(1)
        );

        // Process optimized batches
        let mut futures = Vec::new();
        for (batch_index, batch_operations) in optimized_batches.into_iter().enumerate() {
            if batch_operations.is_empty() {
                continue;
            }

            let batching_service = self.clone();
            let use_locking = use_locking;

            let future = async move {
                let batch_start = Instant::now();

                let result = if use_locking {
                    batching_service
                        .submit_operations_as_locked_batches(batch_operations)
                        .await
                } else {
                    batching_service
                        .submit_operations_as_direct_batches(batch_operations)
                        .await
                };

                let batch_duration = batch_start.elapsed();

                match result {
                    Ok(operation_ids) => {
                        info!(
                            "‚úÖ Adaptive batch {} completed in {:?}: {} operations",
                            batch_index,
                            batch_duration,
                            operation_ids.len()
                        );
                        Ok(operation_ids)
                    }
                    Err(e) => {
                        error!(
                            "‚ùå Adaptive batch {} failed after {:?}: {}",
                            batch_index, batch_duration, e
                        );
                        Err(e)
                    }
                }
            };

            futures.push(future);
        }

        // Wait for all batches to complete
        let total_start = Instant::now();
        let results = futures::future::join_all(futures).await;
        let total_duration = total_start.elapsed();

        // Collect and analyze results
        let mut all_operation_ids = Vec::new();
        let mut successful_batches = 0;
        let mut failed_batches = 0;
        let mut total_successful_operations = 0;

        for result in results {
            match result {
                Ok(operation_ids) => {
                    let operation_ids_len = operation_ids.len();
                    all_operation_ids.extend(operation_ids);
                    successful_batches += 1;
                    total_successful_operations += operation_ids_len;
                }
                Err(_) => {
                    failed_batches += 1;
                }
            }
        }

        info!(
            "üéØ Adaptive super batch processing completed in {:?}: {} successful batches ({} operations), {} failed batches",
            total_duration,
            successful_batches,
            total_successful_operations,
            failed_batches
        );

        Ok(all_operation_ids)
    }

    /// Helper method to analyze operation types for adaptive processing
    fn analyze_operations(&self, operations: &[WriteOperation]) -> OperationAnalysis {
        let mut analysis = OperationAnalysis::default();

        for operation in operations {
            match operation {
                WriteOperation::CreateAccount { .. } => analysis.create_count += 1,
                WriteOperation::DepositMoney { .. } => analysis.deposit_count += 1,
                WriteOperation::WithdrawMoney { .. } => analysis.withdrawal_count += 1,
            }
        }

        analysis
    }

    /// Helper method to determine if locking should be used
    fn should_use_locking(
        &self,
        analysis: &OperationAnalysis,
        config: &AdaptiveBatchConfig,
    ) -> bool {
        // Use locking if there are many operations on the same aggregates
        let total_operations =
            analysis.create_count + analysis.deposit_count + analysis.withdrawal_count;
        let has_mixed_operations = analysis.deposit_count > 0 && analysis.withdrawal_count > 0;

        has_mixed_operations || total_operations > config.locking_threshold
    }

    /// Helper method to calculate optimal batch size
    fn calculate_optimal_batch_size(
        &self,
        analysis: &OperationAnalysis,
        config: &AdaptiveBatchConfig,
    ) -> usize {
        let total_operations =
            analysis.create_count + analysis.deposit_count + analysis.withdrawal_count;

        // Adjust batch size based on operation mix
        let base_size = config.base_batch_size;
        let create_ratio = analysis.create_count as f64 / total_operations as f64;

        if create_ratio > 0.5 {
            // More creates - use smaller batches for better control
            (base_size as f64 * 0.7) as usize
        } else if analysis.deposit_count > analysis.withdrawal_count {
            // More deposits - can use larger batches
            (base_size as f64 * 1.2) as usize
        } else {
            // More withdrawals or mixed - use standard size
            base_size
        }
    }

    /// Helper method to split operations optimally
    fn split_operations_optimally(
        &self,
        operations: Vec<WriteOperation>,
        num_super_batches: usize,
        batch_size_limit: usize,
        analysis: &OperationAnalysis,
    ) -> Vec<Vec<WriteOperation>> {
        let mut batches: Vec<Vec<WriteOperation>> = vec![Vec::new(); num_super_batches];
        let mut current_batch_sizes: Vec<usize> = vec![0; num_super_batches];

        // Sort operations by type for better distribution
        let mut sorted_operations = operations;
        sorted_operations.sort_by(|a, b| {
            // Prioritize creates, then deposits, then withdrawals
            let a_priority = match a {
                WriteOperation::CreateAccount { .. } => 0,
                WriteOperation::DepositMoney { .. } => 1,
                WriteOperation::WithdrawMoney { .. } => 2,
            };
            let b_priority = match b {
                WriteOperation::CreateAccount { .. } => 0,
                WriteOperation::DepositMoney { .. } => 1,
                WriteOperation::WithdrawMoney { .. } => 2,
            };
            a_priority.cmp(&b_priority)
        });

        for operation in sorted_operations {
            // Find the batch with the smallest current size
            let target_batch = current_batch_sizes
                .iter()
                .enumerate()
                .min_by_key(|(_, &size)| size)
                .map(|(index, _)| index)
                .unwrap_or(0);

            // Check if adding this operation would exceed the limit
            if current_batch_sizes[target_batch] < batch_size_limit {
                batches[target_batch].push(operation);
                current_batch_sizes[target_batch] += 1;
            } else {
                // Find next available batch
                let next_batch = current_batch_sizes
                    .iter()
                    .enumerate()
                    .find(|(_, &size)| size < batch_size_limit)
                    .map(|(index, _)| index)
                    .unwrap_or(target_batch);

                batches[next_batch].push(operation);
                current_batch_sizes[next_batch] += 1;
            }
        }

        batches
    }

    /// Submit operations as direct batches to appropriate partitions
    /// This method distributes operations across partitions and processes them as dedicated batches
    /// Use this for high-throughput scenarios with better control over batch processing
    pub async fn submit_operations_as_direct_batches(
        &self,
        operations: Vec<WriteOperation>,
    ) -> Result<Vec<Uuid>> {
        if operations.is_empty() {
            return Ok(Vec::new());
        }

        let operations_len = operations.len();
        info!(
            "üöÄ Starting direct batch processing for {} operations",
            operations_len
        );

        // Group operations by partition
        let mut operations_by_partition: HashMap<usize, Vec<WriteOperation>> = HashMap::new();
        for operation in operations {
            let partition = partition_for_aggregate(&operation.get_aggregate_id(), NUM_PARTITIONS);
            operations_by_partition
                .entry(partition)
                .or_default()
                .push(operation);
        }

        let partitions_count = operations_by_partition.len();
        info!(
            "üì¶ Grouped {} operations across {} partitions",
            operations_len, partitions_count
        );

        // Process each partition's operations as a dedicated batch
        let mut futures = Vec::new();
        for (partition_id, partition_operations) in operations_by_partition {
            let processor = &self.processors[partition_id];
            let operations = partition_operations;

            let future = async move { processor.submit_batch_operations(operations).await };

            futures.push(future);
        }

        // Wait for all partition batches to complete
        let results = futures::future::join_all(futures).await;

        let mut all_operation_ids = Vec::new();
        let mut successful_count = 0;
        let mut failed_count = 0;

        for result in results {
            match result {
                Ok(operation_ids) => {
                    let operation_ids_len = operation_ids.len();
                    all_operation_ids.extend(operation_ids);
                    successful_count += operation_ids_len;
                }
                Err(e) => {
                    error!("Failed to process partition batch: {}", e);
                    failed_count += 1;
                }
            }
        }

        info!(
            "‚úÖ Direct batch processing completed: {} successful operations across {} partitions",
            successful_count, partitions_count
        );

        Ok(all_operation_ids)
    }

    /// Submit operations as locked batches to ensure consistency
    /// This method uses Redis locking to prevent conflicts between operations on the same aggregate
    pub async fn submit_operations_as_locked_batches(
        &self,
        operations: Vec<WriteOperation>,
    ) -> Result<Vec<Uuid>> {
        if operations.is_empty() {
            return Ok(Vec::new());
        }

        let operations_len = operations.len();
        info!(
            "üîí Starting locked batch processing for {} operations",
            operations_len
        );

        // Group operations by partition
        let mut operations_by_partition: HashMap<usize, Vec<WriteOperation>> = HashMap::new();
        for operation in operations {
            let partition = partition_for_aggregate(&operation.get_aggregate_id(), NUM_PARTITIONS);
            operations_by_partition
                .entry(partition)
                .or_default()
                .push(operation);
        }

        let partitions_count = operations_by_partition.len();
        info!(
            "üì¶ Grouped {} operations across {} partitions with locking",
            operations_len, partitions_count
        );

        // Process each partition's operations as a locked batch
        let mut futures = Vec::new();
        for (partition_id, partition_operations) in operations_by_partition {
            let processor = &self.processors[partition_id];
            let operations = partition_operations;

            let future = async move {
                processor
                    .submit_batch_operations_with_locking(operations)
                    .await
            };

            futures.push(future);
        }

        // Wait for all partition batches to complete
        let results = futures::future::join_all(futures).await;

        let mut all_operation_ids = Vec::new();
        let mut successful_count = 0;
        let mut failed_count = 0;

        for result in results {
            match result {
                Ok(operation_ids) => {
                    let operation_ids_len = operation_ids.len();
                    all_operation_ids.extend(operation_ids);
                    successful_count += operation_ids_len;
                }
                Err(e) => {
                    error!("Failed to process locked partition batch: {}", e);
                    failed_count += 1;
                }
            }
        }

        info!(
            "‚úÖ Locked batch processing completed: {} successful operations across {} partitions",
            successful_count, partitions_count
        );

        Ok(all_operation_ids)
    }
}

/// Configuration for write batching
#[derive(Debug, Clone)]
pub struct WriteBatchingConfig {
    pub max_batch_size: usize,
    pub max_batch_wait_time_ms: u64,
    pub max_retries: u32,
    pub retry_backoff_ms: u64,
}

impl Default for WriteBatchingConfig {
    fn default() -> Self {
        Self {
            max_batch_size: CREATE_BATCH_SIZE, // Optimized batch size for better performance
            max_batch_wait_time_ms: DEFAULT_BATCH_TIMEOUT_MS, // Increased from 10ms to 100ms for better batching
            max_retries: 2,      // Reduced from 3 to 2 for faster processing
            retry_backoff_ms: 5, // Reduced from 10ms to 5ms for faster retries
        }
    }
}

/// Types of write operations that can be batched
#[derive(Debug, Clone)]
pub enum WriteOperation {
    CreateAccount {
        account_id: Uuid, // <-- Now required
        owner_name: String,
        initial_balance: Decimal,
    },
    DepositMoney {
        account_id: Uuid,
        amount: Decimal,
    },
    WithdrawMoney {
        account_id: Uuid,
        amount: Decimal,
    },
}

impl WriteOperation {
    pub fn get_aggregate_id(&self) -> Uuid {
        match self {
            WriteOperation::CreateAccount { account_id, .. } => *account_id,
            WriteOperation::DepositMoney { account_id, .. } => *account_id,
            WriteOperation::WithdrawMoney { account_id, .. } => *account_id,
        }
    }
}

/// Result of a write operation
#[derive(Debug, Clone)]
pub struct WriteOperationResult {
    pub operation_id: Uuid,
    pub success: bool,
    pub result: Option<Uuid>, // Account ID for create operations
    pub error: Option<String>,
    pub duration: Duration,
}

/// A batch of write operations
#[derive(Debug)]
pub struct WriteBatch {
    pub batch_id: Uuid,
    pub operations: Vec<(Uuid, WriteOperation)>, // (operation_id, operation)
    pub created_at: Instant,
    pub partition_id: Option<usize>,
}

impl WriteBatch {
    pub fn new(partition_id: Option<usize>) -> Self {
        Self {
            batch_id: Uuid::new_v4(),
            operations: Vec::new(),
            created_at: Instant::now(),
            partition_id: partition_id,
        }
    }

    pub fn add_operation(&mut self, operation_id: Uuid, operation: WriteOperation) {
        self.operations.push((operation_id, operation));
    }

    pub fn is_full(&self, max_size: usize) -> bool {
        self.operations.len() >= max_size
    }

    pub fn is_old(&self, max_wait_time: Duration) -> bool {
        self.created_at.elapsed() >= max_wait_time
    }

    pub fn should_process(&self, max_size: usize, max_wait_time: Duration) -> bool {
        self.is_full(max_size) || self.is_old(max_wait_time)
    }
}

/// Write batching service that groups operations into batches
pub struct WriteBatchingService {
    config: WriteBatchingConfig,
    event_store: Arc<dyn EventStoreTrait>,
    write_pool: Arc<PgPool>,
    outbox_batcher: crate::infrastructure::cdc_debezium::OutboxBatcher,

    // Batching state
    current_batch: Arc<Mutex<WriteBatch>>,
    pending_results: Arc<Mutex<HashMap<Uuid, mpsc::Sender<WriteOperationResult>>>>,

    // FIXED: Store completed results
    completed_results: Arc<Mutex<HashMap<Uuid, WriteOperationResult>>>,

    // Processing
    batch_processor_handle: Arc<Mutex<Option<tokio::task::JoinHandle<()>>>>,
    shutdown_token: CancellationToken,

    // Metrics
    batches_processed: Arc<Mutex<u64>>,
    operations_processed: Arc<Mutex<u64>>,

    // Partitioning state
    partition_id: Option<usize>,
    num_partitions: Option<usize>,
    redis_lock: Arc<RedisAggregateLock>,

    // Bulk insert config manager
    bulk_config_manager: Arc<Mutex<BulkInsertConfigManager>>,
}

impl WriteBatchingService {
    pub fn new(
        config: WriteBatchingConfig,
        event_store: Arc<dyn EventStoreTrait>,
        write_pool: Arc<PgPool>,
    ) -> Self {
        // Create CDC outbox repository
        let outbox_repo = Arc::new(
            crate::infrastructure::cdc_debezium::CDCOutboxRepository::new(
                event_store.get_partitioned_pools().clone(),
            ),
        )
            as Arc<dyn crate::infrastructure::outbox::OutboxRepositoryTrait>;

        Self {
            config,
            event_store: event_store.clone(),
            write_pool: write_pool.clone(),
            outbox_batcher: crate::infrastructure::cdc_debezium::OutboxBatcher::new_default(
                outbox_repo,
                event_store.get_partitioned_pools().clone(),
            ),
            current_batch: Arc::new(Mutex::new(WriteBatch::new(None))),
            pending_results: Arc::new(Mutex::new(HashMap::new())),
            completed_results: Arc::new(Mutex::new(HashMap::new())), // FIXED: Add result storage
            batch_processor_handle: Arc::new(Mutex::new(None)),
            shutdown_token: CancellationToken::new(),
            batches_processed: Arc::new(Mutex::new(0)),
            operations_processed: Arc::new(Mutex::new(0)),
            partition_id: None,
            num_partitions: None,
            redis_lock: Arc::new(RedisAggregateLock::new_legacy("redis://localhost:6379")),
            bulk_config_manager: Arc::new(Mutex::new(BulkInsertConfigManager::new())),
        }
    }

    pub fn new_for_partition(
        partition_id: usize,
        num_partitions: usize,
        config: WriteBatchingConfig,
        event_store: Arc<dyn EventStoreTrait>,
        write_pool: Arc<PgPool>,
    ) -> Self {
        let mut service = Self::new(config, event_store, write_pool);
        service.partition_id = Some(partition_id);
        service.num_partitions = Some(num_partitions);
        // FIXED: Update current_batch with correct partition_id
        service.current_batch = Arc::new(Mutex::new(WriteBatch::new(Some(partition_id))));
        service
    }

    /// Start the batch processor for a partition
    pub async fn start_for_partition(
        partition_id: usize,
        num_partitions: usize,
        config: WriteBatchingConfig,
        event_store: Arc<dyn EventStoreTrait>,
        write_pool: Arc<PgPool>,
    ) -> Result<Self> {
        let mut service = Self::new_for_partition(
            partition_id,
            num_partitions,
            config,
            event_store,
            write_pool,
        );
        service.start().await?;
        Ok(service)
    }

    /// Start the batch processor from an Arc (for use with shared references)
    pub async fn start(&self) -> Result<()> {
        // Check if already running
        if self.batch_processor_handle.lock().await.is_some() {
            return Ok(());
        }

        info!("üöÄ Starting write batching service...");

        let shutdown_token = self.shutdown_token.clone();
        let current_batch = self.current_batch.clone();
        let pending_results = self.pending_results.clone();
        let completed_results = self.completed_results.clone(); // FIXED: Pass completed results
        let config = self.config.clone();
        let event_store = self.event_store.clone();
        let write_pool = self.write_pool.clone();
        let outbox_batcher = self.outbox_batcher.clone();
        let batches_processed = self.batches_processed.clone();
        let operations_processed = self.operations_processed.clone();
        let redis_lock = self.redis_lock.clone();

        let handle = tokio::spawn(async move {
            let mut interval = tokio::time::interval(Duration::from_millis(5));

            loop {
                tokio::select! {
                    _ = interval.tick() => {
                        // Check if current batch should be processed
                        let should_process = {
                            let batch = current_batch.lock().await;
                            let is_full = batch.is_full(config.max_batch_size);
                            let is_old = batch.is_old(Duration::from_millis(config.max_batch_wait_time_ms));
                            let should = batch.should_process(config.max_batch_size, Duration::from_millis(config.max_batch_wait_time_ms));

                            // Only log if there are operations and should process
                            if !batch.operations.is_empty() && should {
                                info!("[WriteBatchingService] Batch processor interval tick");
                                info!("[WriteBatchingService] Batch status: size={}, is_full={}, is_old={}, should_process={}",
                                      batch.operations.len(), is_full, is_old, should);
                            }
                            should
                        };

                        if should_process {
                            // Check if batch has operations before processing
                            let has_operations = {
                                let batch = current_batch.lock().await;
                                !batch.operations.is_empty()
                            };

                            if has_operations {
                                Self::process_current_batch(
                                    &current_batch,
                                    &pending_results,
                                    &completed_results, // FIXED: Pass completed results
                                    &event_store,
                                    &write_pool,
                                    &outbox_batcher,
                                    &config,
                                    &batches_processed,
                                    &operations_processed,
                                    &redis_lock,
                                ).await;
                            }
                        }
                    }
                    _ = shutdown_token.cancelled() => {
                        info!("üõë Write batching service shutdown signal received");
                        break;
                    }
                }
            }

            // Process any remaining operations
            Self::process_current_batch(
                &current_batch,
                &pending_results,
                &completed_results, // FIXED: Pass completed results
                &event_store,
                &write_pool,
                &outbox_batcher,
                &config,
                &batches_processed,
                &operations_processed,
                &redis_lock,
            )
            .await;
        });

        *self.batch_processor_handle.lock().await = Some(handle);

        info!("‚úÖ Write batching service started");
        Ok(())
    }

    /// Stop the batch processor
    pub async fn stop(&mut self) -> Result<()> {
        info!("üõë Stopping write batching service...");

        self.shutdown_token.cancel();

        // Wait for batch processor to complete
        if let Some(handle) = self.batch_processor_handle.lock().await.take() {
            let _ = handle.await;
        }

        info!("‚úÖ Write batching service stopped");
        Ok(())
    }

    /// FIXED: Clean up channels and pending operations
    async fn cleanup_channels(&self) {
        println!("üßπ Cleaning up channels and pending operations...");

        // Process any remaining operations in the current batch
        let remaining_operations = {
            let mut batch = self.current_batch.lock().await;
            if !batch.operations.is_empty() {
                let operations = std::mem::take(&mut batch.operations);
                println!(
                    "üì¶ Processing {} remaining operations during cleanup",
                    operations.len()
                );
                operations
            } else {
                Vec::new()
            }
        };

        if !remaining_operations.is_empty() {
            // Process remaining operations
            Self::process_current_batch(
                &self.current_batch,
                &self.pending_results,
                &self.completed_results,
                &self.event_store,
                &self.write_pool,
                &self.outbox_batcher,
                &self.config,
                &self.batches_processed,
                &self.operations_processed,
                &self.redis_lock,
            )
            .await;
        }

        // Send error results to any remaining pending operations
        let pending_ops: Vec<Uuid> = {
            let pending = self.pending_results.lock().await;
            pending.keys().cloned().collect()
        };

        for operation_id in pending_ops {
            let error_result = WriteOperationResult {
                operation_id,
                success: false,
                result: None,
                error: Some("Service shutdown - operation cancelled".to_string()),
                duration: Duration::ZERO,
            };

            // Store in completed results
            self.completed_results
                .lock()
                .await
                .insert(operation_id, error_result.clone());

            // Try to send to waiting operation
            if let Some(sender) = self.pending_results.lock().await.remove(&operation_id) {
                if let Err(e) = sender.send(error_result).await {
                    println!(
                        "‚ö†Ô∏è  Failed to send shutdown result for operation {}: {}",
                        operation_id, e
                    );
                }
            }
        }

        println!("‚úÖ Channel cleanup completed");
    }

    /// FIXED: Get the number of pending operations (for monitoring)
    pub async fn get_pending_operations_count(&self) -> usize {
        self.pending_results.lock().await.len()
    }

    /// FIXED: Get the number of completed operations (for monitoring)
    pub async fn get_completed_operations_count(&self) -> usize {
        self.completed_results.lock().await.len()
    }

    /// FIXED: Clear old completed results to prevent memory leaks
    pub async fn clear_old_completed_results(&self, max_age_hours: u64) {
        let max_age = Duration::from_secs(max_age_hours * 3600);
        let now = Instant::now();

        let mut completed = self.completed_results.lock().await;
        let initial_count = completed.len();

        // Remove results older than max_age
        completed.retain(|_, result| {
            // Since we don't store timestamps in WriteOperationResult,
            // we'll use a simple count-based cleanup for now
            // In a production system, you'd want to add timestamps to WriteOperationResult
            true // Keep all for now, implement timestamp-based cleanup later
        });

        let removed_count = initial_count - completed.len();
        if removed_count > 0 {
            println!("üßπ Cleaned up {} old completed results", removed_count);
        }
    }

    /// FIXED: Graceful shutdown with proper channel lifecycle management
    pub async fn graceful_shutdown(&self, timeout: Duration) -> Result<()> {
        println!(
            "üîÑ Starting graceful shutdown with {} timeout",
            timeout.as_secs()
        );

        let shutdown_start = Instant::now();

        // Step 1: Stop accepting new operations
        println!("üìù Step 1: Stopping new operation acceptance...");
        // This is handled by the service stopping the batch processor

        // Step 2: Process any remaining operations in the current batch
        println!("üì¶ Step 2: Processing remaining operations...");
        let remaining_count = {
            let batch = self.current_batch.lock().await;
            batch.operations.len()
        };

        if remaining_count > 0 {
            println!(
                "üì¶ Processing {} remaining operations in current batch",
                remaining_count
            );

            // Process the current batch
            Self::process_current_batch(
                &self.current_batch,
                &self.pending_results,
                &self.completed_results,
                &self.event_store,
                &self.write_pool,
                &self.outbox_batcher,
                &self.config,
                &self.batches_processed,
                &self.operations_processed,
                &self.redis_lock,
            )
            .await;
        }

        // Step 3: Wait for all pending operations to complete or timeout
        println!("‚è≥ Step 3: Waiting for pending operations to complete...");
        let mut wait_time = Duration::ZERO;
        let check_interval = Duration::from_millis(100);

        while wait_time < timeout {
            let pending_count = self.get_pending_operations_count().await;
            if pending_count == 0 {
                println!("‚úÖ All pending operations completed");
                break;
            }

            println!(
                "‚è≥ Still waiting for {} pending operations...",
                pending_count
            );
            tokio::time::sleep(check_interval).await;
            wait_time += check_interval;
        }

        // Step 4: Force cleanup of any remaining operations
        if wait_time >= timeout {
            println!("‚ö†Ô∏è  Timeout reached, forcing cleanup of remaining operations");
            self.force_cleanup_remaining_operations().await;
        }

        // Step 5: Final cleanup
        println!("üßπ Step 4: Final cleanup...");
        self.cleanup_channels().await;

        let total_shutdown_time = shutdown_start.elapsed();
        println!(
            "‚úÖ Graceful shutdown completed in {:?}",
            total_shutdown_time
        );

        Ok(())
    }

    /// FIXED: Force cleanup of any remaining operations (for timeout scenarios)
    async fn force_cleanup_remaining_operations(&self) {
        println!("üö® Force cleaning up remaining operations...");

        let pending_ops: Vec<Uuid> = {
            let pending = self.pending_results.lock().await;
            pending.keys().cloned().collect()
        };

        for operation_id in pending_ops {
            let error_result = WriteOperationResult {
                operation_id,
                success: false,
                result: None,
                error: Some("Graceful shutdown timeout - operation cancelled".to_string()),
                duration: Duration::ZERO,
            };

            // Store in completed results
            self.completed_results
                .lock()
                .await
                .insert(operation_id, error_result.clone());

            // Try to send to waiting operation with timeout
            if let Some(sender) = self.pending_results.lock().await.remove(&operation_id) {
                match tokio::time::timeout(Duration::from_millis(100), sender.send(error_result))
                    .await
                {
                    Ok(Ok(_)) => {
                        println!(
                            "‚úÖ Successfully sent timeout result for operation {}",
                            operation_id
                        );
                    }
                    Ok(Err(e)) => {
                        println!(
                            "‚ö†Ô∏è  Failed to send timeout result for operation {}: {}",
                            operation_id, e
                        );
                    }
                    Err(_) => {
                        println!("‚è∞ Timeout sending result for operation {}", operation_id);
                    }
                }
            }
        }

        println!("‚úÖ Force cleanup completed");
    }

    /// FIXED: Check if the service is healthy (for monitoring)
    pub async fn health_check(&self) -> bool {
        // Check if there are any stuck operations
        let pending_count = self.get_pending_operations_count().await;
        let completed_count = self.get_completed_operations_count().await;

        // Service is healthy if pending operations are reasonable (not stuck)
        // and we have processed some operations
        let is_healthy = pending_count < 1000 && completed_count > 0;

        if !is_healthy {
            println!(
                "‚ö†Ô∏è  Health check failed: pending={}, completed={}",
                pending_count, completed_count
            );
        }

        is_healthy
    }

    /// Submit a write operation for batching
    pub async fn submit_operation(
        &self,
        aggregate_id: Uuid,
        operation: WriteOperation,
    ) -> Result<Uuid> {
        let operation_id = Uuid::new_v4();

        println!(
            "üîç [WriteBatchingService] Submitting operation {} for aggregate {}",
            operation_id, aggregate_id
        );

        // FIXED: Increase buffer size to prevent blocking and ensure channel stays open
        let (result_tx, result_rx) = mpsc::channel::<WriteOperationResult>(10);

        // Add to pending results
        self.pending_results
            .lock()
            .await
            .insert(operation_id, result_tx);

        // Add to current batch with proper locking
        {
            let mut batch = self.current_batch.lock().await;
            batch.add_operation(operation_id, operation);

            println!(
                "üîç Batch now has {} operations, max_size: {}, max_wait_time: {}ms",
                batch.operations.len(),
                self.config.max_batch_size,
                self.config.max_batch_wait_time_ms
            );

            // Process immediately if batch is full OR if it's been waiting too long
            if batch.should_process(
                self.config.max_batch_size,
                Duration::from_millis(self.config.max_batch_wait_time_ms),
            ) {
                println!(
                    "üì¶ Processing write batch: {} operations (full: {}, old: {}), age: {:?}",
                    batch.operations.len(),
                    batch.is_full(self.config.max_batch_size),
                    batch.is_old(Duration::from_millis(self.config.max_batch_wait_time_ms)),
                    batch.created_at.elapsed()
                );
                drop(batch); // Release lock before processing

                Self::process_current_batch(
                    &self.current_batch,
                    &self.pending_results,
                    &self.completed_results,
                    &self.event_store,
                    &self.write_pool,
                    &self.outbox_batcher,
                    &self.config,
                    &self.batches_processed,
                    &self.operations_processed,
                    &self.redis_lock,
                )
                .await;
            } else {
                println!(
                    "‚è≥ Batch not ready yet: {} operations, age: {:?}",
                    batch.operations.len(),
                    batch.created_at.elapsed()
                );
            }
        }

        Ok(operation_id)
    }

    /// FIXED: Wait for the result of a submitted operation with improved channel lifecycle
    pub async fn wait_for_result(&self, operation_id: Uuid) -> Result<WriteOperationResult> {
        // First check if result is already completed
        {
            let completed = self.completed_results.lock().await;
            if let Some(result) = completed.get(&operation_id) {
                return Ok(result.clone());
            }
        }

        // Check if operation is still pending
        {
            let pending = self.pending_results.lock().await;
            if !pending.contains_key(&operation_id) {
                // Check completed results again in case it was completed between checks
                drop(pending);
                let completed = self.completed_results.lock().await;
                if let Some(result) = completed.get(&operation_id) {
                    return Ok(result.clone());
                }
                return Err(anyhow::anyhow!(
                    "Operation {} not found or already completed",
                    operation_id
                ));
            }
            // Operation is pending, we'll wait for it
        }

        // Wait for the result with a reasonable timeout
        let timeout = Duration::from_secs(30); // Reasonable timeout for batch processing
        match tokio::time::timeout(timeout, async {
            // Poll for the result more efficiently with exponential backoff
            let mut poll_interval = Duration::from_millis(10);
            let max_poll_interval = Duration::from_millis(100);

            loop {
                // Check completed results first (most likely case)
                {
                    let completed = self.completed_results.lock().await;
                    if let Some(result) = completed.get(&operation_id) {
                        return result.clone();
                    }
                }

                // Check if still pending
                {
                    let pending = self.pending_results.lock().await;
                    if !pending.contains_key(&operation_id) {
                        // Operation completed, check completed results again
                        drop(pending);
                        let completed = self.completed_results.lock().await;
                        if let Some(result) = completed.get(&operation_id) {
                            return result.clone();
                        }
                        // If not found in completed, something went wrong
                        return WriteOperationResult {
                            operation_id,
                            success: false,
                            result: None,
                            error: Some("Operation completed but result not found".to_string()),
                            duration: Duration::ZERO,
                        };
                    }
                }

                // Exponential backoff for polling
                tokio::time::sleep(poll_interval).await;
                poll_interval = std::cmp::min(poll_interval * 2, max_poll_interval);
            }
        })
        .await
        {
            Ok(result) => Ok(result),
            Err(_) => {
                // Clean up any hanging references on timeout
                self.pending_results.lock().await.remove(&operation_id);
                Err(anyhow::anyhow!(
                    "Timeout waiting for operation {} result after 30 seconds",
                    operation_id
                ))
            }
        }
    }

    /// FIXED: Process the current batch of operations
    async fn process_current_batch(
        current_batch: &Arc<Mutex<WriteBatch>>,
        pending_results: &Arc<Mutex<HashMap<Uuid, mpsc::Sender<WriteOperationResult>>>>,
        completed_results: &Arc<Mutex<HashMap<Uuid, WriteOperationResult>>>,
        event_store: &Arc<dyn EventStoreTrait>,
        write_pool: &Arc<PgPool>,
        outbox_batcher: &crate::infrastructure::cdc_debezium::OutboxBatcher,
        config: &WriteBatchingConfig,
        batches_processed: &Arc<Mutex<u64>>,
        operations_processed: &Arc<Mutex<u64>>,
        redis_lock: &Arc<RedisAggregateLock>,
    ) {
        // Take the current batch and create a new one
        let batch_to_process = {
            let mut batch = current_batch.lock().await;
            let batch_to_process = WriteBatch {
                batch_id: batch.batch_id,
                operations: std::mem::take(&mut batch.operations),
                created_at: batch.created_at,
                partition_id: batch.partition_id,
            };
            *batch = WriteBatch::new(batch.partition_id);
            batch_to_process
        };

        if batch_to_process.operations.is_empty() {
            // Silently return for empty batches to reduce log spam
            return;
        }

        println!("üîÑ Starting batch processing with distributed locking...");

        // Group operations by aggregate_id for distributed locking
        let mut ops_by_aggregate: HashMap<Uuid, Vec<(Uuid, WriteOperation)>> = HashMap::new();
        for (op_id, op) in &batch_to_process.operations {
            let aggregate_id = op.get_aggregate_id();
            ops_by_aggregate
                .entry(aggregate_id)
                .or_default()
                .push((*op_id, op.clone()));
        }

        // Process operations with distributed locking
        let mut locked_operations = Vec::new();
        let mut skipped_operations = Vec::new();
        let mut locked_aggregates = Vec::new();

        // Prepare batch lock acquisition
        let mut aggregate_ids: Vec<Uuid> = ops_by_aggregate.keys().cloned().collect();
        // CRITICAL FIX: Sort aggregate_ids for consistent lock ordering
        aggregate_ids.sort();

        let mut operation_types = Vec::new();
        let mut aggregate_to_ops = HashMap::new();

        for aggregate_id in &aggregate_ids {
            if let Some(ops) = ops_by_aggregate.get(aggregate_id) {
                // Determine operation type based on the first operation
                let operation_type = if let Some((_, first_op)) = ops.first() {
                    match first_op {
                        WriteOperation::CreateAccount { .. } => OperationType::Create,
                        WriteOperation::DepositMoney { .. } => OperationType::Update,
                        WriteOperation::WithdrawMoney { .. } => OperationType::Update,
                    }
                } else {
                    OperationType::Write
                };
                operation_types.push(operation_type);
                aggregate_to_ops.insert(*aggregate_id, ops.clone());
            }
        }

        // Try batch lock acquisition
        let lock_results = redis_lock
            .try_batch_lock(aggregate_ids.clone(), operation_types)
            .await;

        // Process results
        for (i, (aggregate_id, lock_acquired)) in
            aggregate_ids.iter().zip(lock_results.iter()).enumerate()
        {
            if *lock_acquired {
                println!(
                    "üîí Acquired lock for aggregate {} (batch mode)",
                    aggregate_id
                );
                if let Some(ops) = aggregate_to_ops.get(aggregate_id) {
                    locked_operations.extend(ops.clone());
                    locked_aggregates.push(*aggregate_id);
                }
            } else {
                println!(
                    "‚ö†Ô∏è Could not acquire lock for aggregate {}, will retry in next batch",
                    aggregate_id
                );
                if let Some(ops) = aggregate_to_ops.get(aggregate_id) {
                    // Instead of skipping, add back to the current batch for retry
                    let mut current_batch = current_batch.lock().await;
                    for (op_id, op) in ops {
                        current_batch.add_operation(*op_id, op.clone());
                    }
                    println!(
                        "üîÑ Re-queued {} operations for aggregate {} in next batch",
                        ops.len(),
                        aggregate_id
                    );
                }
            }
        }

        // Process locked operations in batch
        if !locked_operations.is_empty() {
            let batch_with_locked_ops = WriteBatch {
                batch_id: batch_to_process.batch_id,
                operations: locked_operations,
                created_at: batch_to_process.created_at,
                partition_id: batch_to_process.partition_id,
            };

            println!(
                "üì¶ Processing {} locked operations in batch",
                batch_with_locked_ops.operations.len()
            );

            // Execute the batch with unified transaction
            let batch_results = Self::execute_batch_unified(
                &batch_with_locked_ops,
                event_store,
                write_pool,
                config,
                batch_with_locked_ops.partition_id,
            )
            .await;

            // Store completed results and notify waiting operations
            for (operation_id, result) in batch_results {
                completed_results
                    .lock()
                    .await
                    .insert(operation_id, result.clone());

                if let Some(sender) = pending_results.lock().await.remove(&operation_id) {
                    let _ = sender.send(result).await;
                }
            }
        }

        // Re-queue skipped operations for next batch
        if !skipped_operations.is_empty() {
            println!(
                "üîÑ Re-queuing {} skipped operations for next batch",
                skipped_operations.len()
            );
            let mut batch = current_batch.lock().await;
            for (operation_id, operation) in skipped_operations {
                batch.add_operation(operation_id, operation);
            }
        }

        // Release all locks in batch
        if !locked_aggregates.is_empty() {
            redis_lock.batch_unlock(locked_aggregates).await;
        }

        // Update metrics
        {
            let mut batches = batches_processed.lock().await;
            *batches += 1;
        }
        {
            let mut operations = operations_processed.lock().await;
            *operations += batch_to_process.operations.len() as u64;
        }
    }

    /// Execute a batch of operations in a single database transaction
    pub async fn execute_batch(
        batch: &WriteBatch,
        event_store: &Arc<dyn EventStoreTrait>,
        write_pool: &Arc<PgPool>,
        outbox_batcher: &crate::infrastructure::cdc_debezium::OutboxBatcher,
        config: &WriteBatchingConfig,
        partition_id: Option<usize>,
    ) -> Vec<(Uuid, WriteOperationResult)> {
        println!(
            "[DEBUG] execute_batch: Starting batch with {} operations",
            batch.operations.len()
        );
        let mut results = Vec::new();
        let mut retry_count = 0;

        // BULK MODE: Batch boyutu b√ºy√ºkse bulk mode ba≈ülat (threshold'u d√º≈ü√ºrd√ºk)
        let should_use_bulk_mode = batch.operations.len() >= 3; // 3+ operation i√ßin bulk mode (5'ten 3'e d√º≈ü√ºrd√ºk)
        let mut bulk_config_manager = if should_use_bulk_mode {
            Some(BulkInsertConfigManager::new())
        } else {
            None
        };

        // BULK MODE: Eƒüer bulk mode kullanƒ±lacaksa ba≈ülat
        if let Some(ref mut config_manager) = bulk_config_manager {
            if let Err(e) = config_manager
                .start_bulk_mode_event_store(event_store)
                .await
            {
                error!("Failed to start bulk mode: {}", e);
                // Bulk mode ba≈üarƒ±sƒ±z olsa bile normal modda devam et
            } else {
                info!(
                    "üöÄ Bulk mode activated for batch with {} operations",
                    batch.operations.len()
                );
            }
        }

        while retry_count < config.max_retries {
            // Always start a new transaction for each retry attempt
            let mut transaction = match write_pool.begin().await {
                Ok(tx) => tx,
                Err(e) => {
                    retry_count += 1;
                    if retry_count >= config.max_retries {
                        for (operation_id, _) in &batch.operations {
                            results.push((
                                *operation_id,
                                WriteOperationResult {
                                    operation_id: *operation_id,
                                    success: false,
                                    result: None,
                                    error: Some(format!(
                                        "Failed to begin transaction after {} retries: {}",
                                        config.max_retries, e
                                    )),
                                    duration: Duration::ZERO,
                                },
                            ));
                        }
                        return results;
                    }
                    continue;
                }
            };

            let batch_start = Instant::now();

            // Collect all events and outbox messages from all operations
            let mut all_events = Vec::new();
            let mut all_outbox_messages = Vec::new();
            let mut operation_results = Vec::new();

            for (operation_id, operation) in &batch.operations {
                let op_start = Instant::now();
                let (events, outbox_messages, result_id) = match operation {
                    WriteOperation::CreateAccount {
                        account_id,
                        owner_name,
                        initial_balance,
                    } => Self::prepare_create_account_operation(
                        *account_id,
                        owner_name,
                        *initial_balance,
                    ),
                    WriteOperation::DepositMoney { account_id, amount } => {
                        Self::prepare_deposit_money_operation(*account_id, *amount)
                    }
                    WriteOperation::WithdrawMoney { account_id, amount } => {
                        Self::prepare_withdraw_money_operation(*account_id, *amount)
                    }
                };

                all_events.extend(events);
                all_outbox_messages.extend(outbox_messages);

                operation_results.push((*operation_id, result_id, op_start.elapsed()));
            }

            info!(
                "[DEBUG] execute_batch: Prepared {} events and {} outbox messages",
                all_events.len(),
                all_outbox_messages.len()
            );

            // Group events by aggregate_id for efficient batch insertion
            let mut events_by_aggregate: Vec<(Uuid, Vec<AccountEvent>, Option<usize>)> = Vec::new();

            // Group events by aggregate_id
            for event in all_events {
                let aggregate_id = match &event {
                    AccountEvent::AccountCreated { account_id, .. } => *account_id,
                    AccountEvent::MoneyDeposited { account_id, .. } => *account_id,
                    AccountEvent::MoneyWithdrawn { account_id, .. } => *account_id,
                    AccountEvent::AccountClosed { account_id, .. } => *account_id,
                };

                // Find or create entry for this aggregate
                let entry = events_by_aggregate
                    .iter_mut()
                    .find(|(id, _, _)| *id == aggregate_id);

                match entry {
                    Some((_, events, _)) => {
                        // For subsequent events on the same aggregate, just add the event
                        // Version will be determined by event_store
                        events.push(event);
                    }
                    None => {
                        // For new aggregates, version will be determined by event_store
                        events_by_aggregate.push((aggregate_id, vec![event], partition_id));
                    }
                }
            }

            println!("[DEBUG] execute_batch: About to call save_events_multi_aggregate_in_transaction with {} aggregates", events_by_aggregate.len());
            let result = event_store
                .save_events_multi_aggregate_in_transaction(&mut transaction, events_by_aggregate)
                .await;
            println!(
                "[DEBUG] execute_batch: Transaction complete, results: {:?}",
                result
            );

            if let Err(e) = result {
                let partition_info = partition_id
                    .map(|p| format!("partition_{}", p))
                    .unwrap_or_else(|| "unknown_partition".to_string());
                println!(
                    "[DEBUG] execute_batch: Failed to save events in multi-aggregate batch ({}): {:?}",
                    partition_info, e
                );
                error!(
                    "Failed to save events in multi-aggregate batch ({}): {:?}",
                    partition_info, e
                );

                // Check if this is a serialization conflict that we should retry
                let is_serialization_conflict = e
                    .to_string()
                    .contains("could not serialize access due to read/write dependencies");

                if is_serialization_conflict && retry_count < config.max_retries - 1 {
                    // Rollback transaction and retry with exponential backoff
                    let _ = transaction.rollback().await;
                    retry_count += 1;
                    let backoff = Duration::from_millis(50 * retry_count as u64); // Use fixed backoff
                    info!(
                        "[DEBUG] Serialization conflict detected, retrying in {:?} (attempt {}/{})",
                        backoff, retry_count, config.max_retries
                    );
                    tokio::time::sleep(backoff).await;
                    continue;
                } else {
                    // Rollback transaction on any error
                    let _ = transaction.rollback().await;
                    retry_count += 1;
                    if retry_count >= config.max_retries {
                        // Return failure results for all operations
                        for (operation_id, _, _) in operation_results {
                            results.push((
                                operation_id,
                                WriteOperationResult {
                                    operation_id,
                                    success: false,
                                    result: None,
                                    error: Some(format!(
                                        "Failed to insert events after {} retries: {}",
                                        config.max_retries, e
                                    )),
                                    duration: Duration::ZERO,
                                },
                            ));
                        }
                        return results;
                    }
                    continue;
                }
            }

            // Commit transaction FIRST
            if let Err(e) = transaction.commit().await {
                retry_count += 1;
                if retry_count >= config.max_retries {
                    for (operation_id, _, _) in operation_results {
                        results.push((
                            operation_id,
                            WriteOperationResult {
                                operation_id,
                                success: false,
                                result: None,
                                error: Some(format!(
                                    "Failed to commit transaction after {} retries: {}",
                                    config.max_retries, e
                                )),
                                duration: Duration::ZERO,
                            },
                        ));
                    }
                    return results;
                }
                continue;
            }

            // Transaction committed successfully, now process outbox messages
            let outbox_start = Instant::now();
            let mut outbox_success = true;

            // OPTIMIZED: Use bulk submit instead of individual submissions
            if !all_outbox_messages.is_empty() {
                // Choose optimal method based on batch size
                if all_outbox_messages.len() >= 100 {
                    // For large batches, use direct bulk insert for maximum performance
                    if let Err(e) = WriteBatchingService::submit_outbox_messages_direct_bulk(
                        write_pool.clone(),
                        &outbox_batcher,
                        all_outbox_messages.clone(),
                    )
                    .await
                    {
                        error!(
                            "Failed to submit outbox messages via direct bulk insert: {}",
                            e
                        );
                        outbox_success = false;
                    }
                } else {
                    // For smaller batches, use OutboxBatcher for better batching
                    if let Err(e) = outbox_batcher
                        .submit_bulk(all_outbox_messages.clone())
                        .await
                    {
                        error!("Failed to submit outbox messages in bulk: {}", e);
                        outbox_success = false;
                    }
                }
            }

            let outbox_duration = outbox_start.elapsed();

            if !outbox_success {
                error!("Failed to submit some outbox messages");
                // Don't fail the entire batch for outbox errors, just log them
            }

            info!(
                "[DEBUG] execute_batch: Completed successfully - {} operations, {} outbox messages, outbox_success={}",
                batch.operations.len(),
                all_outbox_messages.len(),
                outbox_success
            );

            // Create success results for all operations
            for (operation_id, result_id, op_duration) in operation_results {
                results.push((
                    operation_id,
                    WriteOperationResult {
                        operation_id,
                        success: true,
                        result: Some(result_id),
                        error: None,
                        duration: op_duration,
                    },
                ));
            }

            println!(
                "[DEBUG] execute_batch: Finished batch with {} results",
                results.len()
            );

            // BULK MODE: ƒ∞≈ülem ba≈üarƒ±lƒ± olduysa bulk mode'u sonlandƒ±r
            if let Some(ref mut config_manager) = bulk_config_manager {
                if let Err(e) = config_manager.end_bulk_mode_event_store(event_store).await {
                    error!("Failed to end bulk mode: {}", e);
                } else {
                    info!("üîÑ Bulk mode deactivated after successful batch processing");
                }
            }

            return results;
        }

        // BULK MODE: T√ºm retry'lar ba≈üarƒ±sƒ±z olduysa bulk mode'u sonlandƒ±r
        if let Some(ref mut config_manager) = bulk_config_manager {
            if let Err(e) = config_manager.end_bulk_mode_event_store(event_store).await {
                error!("Failed to end bulk mode after retry failures: {}", e);
            }
        }

        // If we get here, all retries failed
        println!(
            "[DEBUG] execute_batch: All retries failed, returning {} failure results",
            batch.operations.len()
        );
        results
    }

    // Helper methods to prepare operations without executing them
    fn prepare_create_account_operation(
        account_id: Uuid,
        owner_name: &str,
        initial_balance: Decimal,
    ) -> (Vec<AccountEvent>, Vec<OutboxMessage>, Uuid) {
        let event = AccountEvent::AccountCreated {
            account_id,
            owner_name: owner_name.to_string(),
            initial_balance,
        };

        let outbox_message = OutboxMessage {
            aggregate_id: account_id,
            event_id: Uuid::new_v4(),
            event_type: "AccountCreated".to_string(),
            payload: bincode::serialize(&event).unwrap_or_default(),
            topic: "banking-es.public.kafka_outbox_cdc".to_string(), // FIXED: Use correct CDC topic format
            metadata: None,
        };

        (vec![event], vec![outbox_message], account_id)
    }

    fn prepare_deposit_money_operation(
        account_id: Uuid,
        amount: Decimal,
    ) -> (Vec<AccountEvent>, Vec<OutboxMessage>, Uuid) {
        let event = AccountEvent::MoneyDeposited {
            account_id,
            amount,
            transaction_id: Uuid::new_v4(),
        };

        let outbox_message = OutboxMessage {
            aggregate_id: account_id,
            event_id: Uuid::new_v4(),
            event_type: "MoneyDeposited".to_string(),
            payload: bincode::serialize(&event).unwrap_or_default(),
            topic: "banking-es.public.kafka_outbox_cdc".to_string(), // FIXED: Use correct CDC topic format
            metadata: None,
        };

        (vec![event], vec![outbox_message], account_id)
    }

    fn prepare_withdraw_money_operation(
        account_id: Uuid,
        amount: Decimal,
    ) -> (Vec<AccountEvent>, Vec<OutboxMessage>, Uuid) {
        let event = AccountEvent::MoneyWithdrawn {
            account_id,
            amount,
            transaction_id: Uuid::new_v4(),
        };

        let outbox_message = OutboxMessage {
            aggregate_id: account_id,
            event_id: Uuid::new_v4(),
            event_type: "MoneyWithdrawn".to_string(),
            payload: bincode::serialize(&event).unwrap_or_default(),
            topic: "banking-es.public.kafka_outbox_cdc".to_string(), // FIXED: Use correct CDC topic format
            metadata: None,
        };

        (vec![event], vec![outbox_message], account_id)
    }

    /// Get service statistics with channel lifecycle monitoring
    pub async fn get_stats(&self) -> serde_json::Value {
        let batches_processed = *self.batches_processed.lock().await;
        let operations_processed = *self.operations_processed.lock().await;
        let pending_count = self.get_pending_operations_count().await;
        let completed_count = self.get_completed_operations_count().await;

        // Get Redis lock metrics
        let lock_metrics = self.redis_lock.get_metrics_json();

        serde_json::json!({
            "batches_processed": batches_processed,
            "operations_processed": operations_processed,
            "pending_operations": pending_count,
            "completed_operations": completed_count,
            "redis_lock_metrics": lock_metrics,
            "partition_id": self.partition_id,
            "num_partitions": self.num_partitions,
        })
    }

    /// Get the current batch size (for smart consistency management)
    pub async fn get_current_batch_size(&self) -> usize {
        self.current_batch.lock().await.operations.len()
    }

    /// Submit a batch of operations directly to this processor
    /// This method bypasses the normal batching queue and processes operations immediately
    /// Use this for high-throughput scenarios where you want direct control over batch processing
    pub async fn submit_batch_operations(
        &self,
        operations: Vec<WriteOperation>,
    ) -> Result<Vec<Uuid>> {
        if operations.is_empty() {
            return Ok(Vec::new());
        }

        info!(
            "üöÄ [WriteBatchingService] Submitting {} operations as direct batch",
            operations.len()
        );

        // Create a dedicated batch for immediate processing
        let mut dedicated_batch = WriteBatch::new(self.partition_id);
        let mut operation_ids = Vec::with_capacity(operations.len());

        // Add all operations to the batch
        for operation in operations {
            let operation_id = Uuid::new_v4();
            dedicated_batch.add_operation(operation_id, operation);
            operation_ids.push(operation_id);
        }

        info!(
            "üì¶ Created dedicated batch with {} operations for partition {:?}",
            dedicated_batch.operations.len(),
            self.partition_id
        );

        // Execute the batch directly with unified transaction
        let batch_start = Instant::now();
        let batch_results = Self::execute_batch_unified(
            &dedicated_batch,
            &self.event_store,
            &self.write_pool,
            &self.config,
            dedicated_batch.partition_id,
        )
        .await;

        let batch_duration = batch_start.elapsed();
        let successful_count = batch_results.iter().filter(|(_, r)| r.success).count();

        info!(
            "‚úÖ Direct batch processing completed in {:?}: {}/{} successful operations",
            batch_duration,
            successful_count,
            batch_results.len()
        );

        // Store completed results for potential future queries
        for (operation_id, result) in batch_results {
            self.completed_results
                .lock()
                .await
                .insert(operation_id, result);
        }

        // Return operation IDs (both successful and failed)
        Ok(operation_ids)
    }

    /// Submit a batch of operations with Redis locking for consistency
    /// This method ensures that operations on the same aggregate_id are processed atomically
    pub async fn submit_batch_operations_with_locking(
        &self,
        operations: Vec<WriteOperation>,
    ) -> Result<Vec<Uuid>> {
        if operations.is_empty() {
            return Ok(Vec::new());
        }

        info!(
            "üîí [WriteBatchingService] Submitting {} operations with Redis locking",
            operations.len()
        );

        // Group operations by aggregate_id for proper locking
        let mut operations_by_aggregate: HashMap<Uuid, Vec<WriteOperation>> = HashMap::new();
        for operation in operations {
            let aggregate_id = operation.get_aggregate_id();
            operations_by_aggregate
                .entry(aggregate_id)
                .or_default()
                .push(operation);
        }

        // Prepare for batch locking
        let aggregate_ids: Vec<Uuid> = operations_by_aggregate.keys().cloned().collect();
        let operation_types: Vec<OperationType> = aggregate_ids
            .iter()
            .map(|_| OperationType::Write) // Default to Write for mixed operations
            .collect();

        // Try to acquire locks for all aggregates
        let lock_results = self
            .redis_lock
            .try_batch_lock(aggregate_ids.clone(), operation_types)
            .await;

        // Process only operations for aggregates where we got the lock
        let mut locked_operations = Vec::new();
        let mut locked_aggregates = Vec::new();

        for (i, (aggregate_id, lock_acquired)) in
            aggregate_ids.iter().zip(lock_results.iter()).enumerate()
        {
            if *lock_acquired {
                if let Some(ops) = operations_by_aggregate.get(aggregate_id) {
                    locked_operations.extend(ops.clone());
                    locked_aggregates.push(*aggregate_id);
                    info!(
                        "üîí Acquired lock for aggregate {} ({} operations)",
                        aggregate_id,
                        ops.len()
                    );
                }
            } else {
                warn!(
                    "‚ö†Ô∏è Could not acquire lock for aggregate {}, skipping {} operations",
                    aggregate_id,
                    operations_by_aggregate
                        .get(aggregate_id)
                        .map(|ops| ops.len())
                        .unwrap_or(0)
                );
            }
        }

        if locked_operations.is_empty() {
            warn!("‚ö†Ô∏è No operations could be processed due to lock conflicts");
            return Ok(Vec::new());
        }

        // Create batch for locked operations
        let mut dedicated_batch = WriteBatch::new(self.partition_id);
        let mut operation_ids = Vec::with_capacity(locked_operations.len());

        for operation in locked_operations {
            let operation_id = Uuid::new_v4();
            dedicated_batch.add_operation(operation_id, operation);
            operation_ids.push(operation_id);
        }

        // Execute the batch with unified transaction
        let batch_start = Instant::now();
        let batch_results = Self::execute_batch_unified(
            &dedicated_batch,
            &self.event_store,
            &self.write_pool,
            &self.config,
            dedicated_batch.partition_id,
        )
        .await;

        let batch_duration = batch_start.elapsed();
        let successful_count = batch_results.iter().filter(|(_, r)| r.success).count();

        info!(
            "‚úÖ Locked batch processing completed in {:?}: {}/{} successful operations",
            batch_duration,
            successful_count,
            batch_results.len()
        );

        // Store completed results
        for (operation_id, result) in batch_results {
            self.completed_results
                .lock()
                .await
                .insert(operation_id, result);
        }

        // Release all locks
        if !locked_aggregates.is_empty() {
            self.redis_lock.batch_unlock(locked_aggregates).await;
        }

        Ok(operation_ids)
    }

    /// Direct bulk insert method for maximum performance with large batches
    /// This bypasses the OutboxBatcher and inserts directly into the database
    pub async fn submit_outbox_messages_direct_bulk(
        write_pool: Arc<PgPool>,
        outbox_batcher: &crate::infrastructure::cdc_debezium::OutboxBatcher,
        messages: Vec<crate::infrastructure::outbox::OutboxMessage>,
    ) -> Result<()> {
        if messages.is_empty() {
            return Ok(());
        }

        let messages_len = messages.len();
        let start_time = Instant::now();
        info!(
            "üöÄ Starting direct bulk insert of {} outbox messages",
            messages_len
        );

        let mut transaction = write_pool.begin().await.map_err(|e| {
            anyhow::anyhow!("Failed to begin transaction for direct bulk insert: {}", e)
        })?;

        // Use the CDC repository's bulk insert method directly
        // The repository will automatically choose COPY for large batches
        let cdc_repo = crate::infrastructure::cdc_debezium::CDCOutboxRepository::new(Arc::new(
            crate::infrastructure::PartitionedPools {
                write_pool: (*write_pool).clone(),
                read_pool: (*write_pool).clone(),
                config: crate::infrastructure::PoolPartitioningConfig::default(),
            },
        ));

        if let Err(e) = cdc_repo
            .add_pending_messages_copy(&mut transaction, messages)
            .await
        {
            error!("Failed to bulk insert outbox messages directly: {}", e);
            transaction
                .rollback()
                .await
                .map_err(|re| anyhow::anyhow!("Failed to rollback transaction: {}", re))?;
            return Err(anyhow::anyhow!("Direct bulk insert failed: {}", e));
        }

        transaction.commit().await.map_err(|e| {
            anyhow::anyhow!("Failed to commit direct bulk insert transaction: {}", e)
        })?;

        let duration = start_time.elapsed();
        info!(
            "‚úÖ Direct bulk insert completed: {} messages in {:?}",
            messages_len, duration
        );

        Ok(())
    }

    /// Bulk insert modunu ba≈ülat
    pub async fn start_bulk_mode(&self) -> Result<()> {
        let mut config_manager = BulkInsertConfigManager::new();
        config_manager
            .start_bulk_mode_event_store(&self.event_store)
            .await
    }

    /// Bulk insert modunu sonlandƒ±r
    pub async fn end_bulk_mode(&self) -> Result<()> {
        let mut config_manager = BulkInsertConfigManager::new();
        config_manager
            .end_bulk_mode_event_store(&self.event_store)
            .await
    }

    /// Bulk modda olup olmadƒ±ƒüƒ±nƒ± kontrol et
    pub async fn is_bulk_mode(&self) -> bool {
        let config_manager = self.bulk_config_manager.lock().await;
        config_manager.is_bulk_mode()
    }

    /// Bulk insert ile batch i≈ülemleri yap
    pub async fn execute_batch_with_bulk_config(
        &self,
        batch: &WriteBatch,
    ) -> Result<Vec<(Uuid, WriteOperationResult)>> {
        // Bulk modu ba≈ülat
        self.start_bulk_mode().await?;

        let results = Self::execute_batch_unified(
            batch,
            &self.event_store,
            &self.write_pool,
            &self.config,
            batch.partition_id,
        )
        .await;

        // Bulk modu sonlandƒ±r
        self.end_bulk_mode().await?;

        Ok(results)
    }

    /// Bulk insert ile batch operations submit et
    pub async fn submit_batch_operations_with_bulk_config(
        &self,
        operations: Vec<WriteOperation>,
    ) -> Result<Vec<Uuid>> {
        // Bulk modu ba≈ülat
        self.start_bulk_mode().await?;

        let operation_ids = self.submit_batch_operations(operations).await?;

        // Bulk modu sonlandƒ±r
        self.end_bulk_mode().await?;

        Ok(operation_ids)
    }

    /// Execute batch with unified transaction - both events and outbox inserts in single transaction
    /// This provides better atomicity and performance compared to separate transactions
    // This helper function now contains the core logic that needs to be async
    async fn execute_batch_unified_inner(
        batch: &WriteBatch,
        event_store: &Arc<dyn EventStoreTrait>,
        write_pool: &Arc<PgPool>,
        config: &WriteBatchingConfig,
        partition_id: Option<usize>,
        retry_count: usize,
    ) -> Result<Vec<(Uuid, WriteOperationResult)>, String> {
        // Start unified transaction for both events and outbox
        let mut transaction = write_pool
            .begin()
            .await
            .map_err(|e| format!("Failed to begin unified transaction: {}", e))?;

        let batch_start = Instant::now();

        // Collect all events and outbox messages from all operations
        let mut all_events = Vec::new();
        let mut all_outbox_messages = Vec::new();
        let mut operation_results = Vec::new();

        for (operation_id, operation) in &batch.operations {
            let op_start = Instant::now();
            let (events, outbox_messages, result_id) = match operation {
                WriteOperation::CreateAccount {
                    account_id,
                    owner_name,
                    initial_balance,
                } => Self::prepare_create_account_operation(
                    *account_id,
                    owner_name,
                    *initial_balance,
                ),
                WriteOperation::DepositMoney { account_id, amount } => {
                    Self::prepare_deposit_money_operation(*account_id, *amount)
                }
                WriteOperation::WithdrawMoney { account_id, amount } => {
                    Self::prepare_withdraw_money_operation(*account_id, *amount)
                }
            };

            all_events.extend(events);
            all_outbox_messages.extend(outbox_messages);

            operation_results.push((*operation_id, result_id, op_start.elapsed()));
        }

        info!(
            "[DEBUG] execute_batch_unified: Prepared {} events and {} outbox messages",
            all_events.len(),
            all_outbox_messages.len()
        );

        // Group events by aggregate_id for efficient batch insertion
        let mut events_by_aggregate: Vec<(Uuid, Vec<AccountEvent>, Option<usize>)> = Vec::new();

        // Group events by aggregate_id
        for event in all_events {
            let aggregate_id = match &event {
                AccountEvent::AccountCreated { account_id, .. } => *account_id,
                AccountEvent::MoneyDeposited { account_id, .. } => *account_id,
                AccountEvent::MoneyWithdrawn { account_id, .. } => *account_id,
                AccountEvent::AccountClosed { account_id, .. } => *account_id,
            };

            // Find or create entry for this aggregate
            let entry = events_by_aggregate
                .iter_mut()
                .find(|(id, _, _)| *id == aggregate_id);

            match entry {
                Some((_, events, _)) => {
                    // For subsequent events on the same aggregate, just add the event
                    events.push(event);
                }
                None => {
                    // For new aggregates, version will be determined by event_store
                    events_by_aggregate.push((aggregate_id, vec![event], partition_id));
                }
            }
        }

        println!("[DEBUG] execute_batch_unified: About to execute parallel operations with {} aggregates and {} outbox messages", events_by_aggregate.len(), all_outbox_messages.len());

        let final_result: Result<(), String> = if all_outbox_messages.is_empty() {
            // If no outbox messages, only execute events operation
            event_store
                .save_events_multi_aggregate_in_transaction(&mut transaction, events_by_aggregate)
                .await
                .map_err(|e| e.to_string())
        } else {
            // Execute events and outbox operations sequentially within the same transaction
            let events_res = event_store
                .save_events_multi_aggregate_in_transaction(&mut transaction, events_by_aggregate)
                .await
                .map_err(|e| e.to_string());

            let outbox_res = if events_res.is_ok() {
                let cdc_repo = crate::infrastructure::cdc_debezium::CDCOutboxRepository::new(
                    Arc::new(crate::infrastructure::PartitionedPools {
                        write_pool: (**write_pool).clone(),
                        read_pool: (**write_pool).clone(),
                        config: crate::infrastructure::PoolPartitioningConfig::default(),
                    }),
                );
                cdc_repo
                    .add_pending_messages_copy(&mut transaction, all_outbox_messages.clone())
                    .await
                    .map_err(|e| e.to_string())
            } else {
                Err("Events operation failed, skipping outbox".to_string())
            };

            // Handle results - fail if either fails
            match (events_res, outbox_res) {
                (Ok(_), Ok(_)) => Ok(()),
                (Err(e), _) => Err(e),
                (_, Err(e)) => Err(e),
            }
        };

        // Commit or rollback based on final_result
        if final_result.is_ok() {
            transaction.commit().await.map_err(|e| e.to_string())?;
            println!("[DEBUG] execute_batch_unified: Successfully completed operations and committed transaction");
            info!(
            "[DEBUG] execute_batch_unified: Completed successfully - {} operations, {} outbox messages in unified transaction",
            batch.operations.len(),
            all_outbox_messages.len()
        );
        } else {
            transaction.rollback().await.map_err(|e| e.to_string())?;
            println!("[DEBUG] execute_batch_unified: Operations failed, rolled back transaction");
        }

        // Create success/failure results based on final_result
        let final_op_results = operation_results
            .into_iter()
            .map(|(operation_id, result_id, op_duration)| {
                if final_result.is_ok() {
                    (
                        operation_id,
                        WriteOperationResult {
                            operation_id,
                            success: true,
                            result: Some(result_id),
                            error: None,
                            duration: op_duration,
                        },
                    )
                } else {
                    (
                        operation_id,
                        WriteOperationResult {
                            operation_id,
                            success: false,
                            result: None,
                            error: final_result.as_ref().err().cloned(),
                            duration: op_duration,
                        },
                    )
                }
            })
            .collect();

        Ok(final_op_results)
    }

    pub async fn execute_batch_unified(
        batch: &WriteBatch,
        event_store: &Arc<dyn EventStoreTrait>,
        write_pool: &Arc<PgPool>,
        config: &WriteBatchingConfig,
        partition_id: Option<usize>,
    ) -> Vec<(Uuid, WriteOperationResult)> {
        let mut results = Vec::new();
        let mut retry_count = 0;

        // BULK MODE: Batch size is large, start bulk mode
        let should_use_bulk_mode = batch.operations.len() >= 3;
        let mut bulk_config_manager = if should_use_bulk_mode {
            Some(BulkInsertConfigManager::new())
        } else {
            None
        };

        // BULK MODE: Start bulk mode if it will be used
        if let Some(ref mut config_manager) = bulk_config_manager {
            if let Err(e) = config_manager
                .start_bulk_mode_event_store(event_store)
                .await
            {
                error!("Failed to start bulk mode: {}", e);
                // Continue in normal mode even if bulk mode fails to start
            } else {
                info!(
                    "üöÄ Bulk mode activated for unified batch with {} operations",
                    batch.operations.len()
                );
            }
        }

        loop {
            match Self::execute_batch_unified_inner(
                batch,
                event_store,
                write_pool,
                config,
                partition_id,
                retry_count,
            )
            .await
            {
                Ok(op_results) => {
                    // Success path
                    results = op_results;
                    println!(
                        "[DEBUG] execute_batch_unified: Finished unified batch with {} results",
                        results.len()
                    );

                    // BULK MODE: End bulk mode if the operation was successful
                    if let Some(ref mut config_manager) = bulk_config_manager {
                        if let Err(e) = config_manager.end_bulk_mode_event_store(event_store).await
                        {
                            error!("Failed to end bulk mode: {}", e);
                        } else {
                            info!("üîÑ Bulk mode deactivated after successful unified batch processing");
                        }
                    }
                    return results;
                }
                Err(error_msg) => {
                    let is_serialization_conflict = error_msg
                        .contains("could not serialize access due to read/write dependencies");
                    let is_duplicate_key_error =
                        error_msg.contains("duplicate key value violates unique constraint");

                    if (is_serialization_conflict || is_duplicate_key_error)
                        && retry_count < config.max_retries as usize - 1
                    {
                        retry_count += 1;
                        let backoff = Duration::from_millis(100 * retry_count as u64);
                        info!(
                            "[DEBUG] {} detected, retrying in {:?} (attempt {}/{})",
                            if is_duplicate_key_error {
                                "Duplicate key error"
                            } else {
                                "Serialization conflict"
                            },
                            backoff,
                            retry_count,
                            config.max_retries
                        );
                        tokio::time::sleep(backoff).await;
                        continue; // Continue the loop for a retry
                    } else {
                        // All retries failed or it's a non-retriable error
                        error!("Failed after all retries: {}", error_msg);

                        // BULK MODE: If all retries failed, end bulk mode
                        if let Some(ref mut config_manager) = bulk_config_manager {
                            if let Err(e) =
                                config_manager.end_bulk_mode_event_store(event_store).await
                            {
                                error!("Failed to end bulk mode after retry failures: {}", e);
                            }
                        }

                        println!(
                        "[DEBUG] execute_batch_unified: All retries failed, returning {} failure results",
                        batch.operations.len()
                    );

                        for (operation_id, _) in &batch.operations {
                            results.push((
                                *operation_id,
                                WriteOperationResult {
                                    operation_id: *operation_id,
                                    success: false,
                                    result: None,
                                    error: Some(error_msg.clone()),
                                    duration: Duration::ZERO,
                                },
                            ));
                        }
                        return results;
                    }
                }
            }
        }
    }
}
/// Helper to check if an error is a version conflict
fn is_version_conflict(e: &crate::infrastructure::event_store::EventStoreError) -> bool {
    let msg = format!("{}", e);
    msg.contains("Invalid version sequence")
        || msg.contains(
            "could not serialize access due to read/write dependencies among transactions",
        )
}

/// Robust event writing with version conflict retry
pub async fn save_events_with_retry(
    event_store: &Arc<dyn EventStoreTrait>,
    aggregate_id: Uuid,
    events: Vec<AccountEvent>,
    max_retries: usize,
) -> Result<(), crate::infrastructure::event_store::EventStoreError> {
    let mut retries = 0;
    let mut backoff = 100;

    loop {
        // Get current version on each attempt to avoid race conditions
        let current_version = event_store.get_current_version(aggregate_id, false).await?;

        let result = event_store
            .save_events(aggregate_id, events.clone(), current_version)
            .await;

        match result {
            Ok(_) => return Ok(()),
            Err(e) if is_version_conflict(&e) && retries < max_retries => {
                retries += 1;
                println!("[DEBUG] Version conflict for aggregate_id={:?}, retrying (attempt {}/{}), current_version={}", 
                    aggregate_id, retries, max_retries, current_version);
                let jitter = rand::thread_rng().gen_range(0..50);
                tokio::time::sleep(std::time::Duration::from_millis(backoff + jitter)).await;
                backoff = std::cmp::min(backoff * 2, 1000); // Cap backoff at 1 second
            }
            Err(e) => return Err(e),
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_write_batching_config() {
        let config = WriteBatchingConfig::default();
        assert_eq!(config.max_batch_size, 1000);
        assert_eq!(config.max_batch_wait_time_ms, 50);
        assert_eq!(config.max_retries, 2);
    }

    #[tokio::test]
    async fn test_write_batch_creation() {
        let mut batch = WriteBatch::new(Some(0));
        assert_eq!(batch.operations.len(), 0);

        batch.add_operation(
            Uuid::new_v4(),
            WriteOperation::CreateAccount {
                account_id: Uuid::new_v4(),
                owner_name: "Test User".to_string(),
                initial_balance: Decimal::new(1000, 0),
            },
        );

        assert_eq!(batch.operations.len(), 1);
    }

    #[tokio::test]
    async fn test_write_batch_should_process() {
        let mut batch = WriteBatch::new(Some(0));

        // Test size-based processing
        for _ in 0..50 {
            batch.add_operation(
                Uuid::new_v4(),
                WriteOperation::CreateAccount {
                    account_id: Uuid::new_v4(),
                    owner_name: "Test User".to_string(),
                    initial_balance: Decimal::new(1000, 0),
                },
            );
        }

        assert!(batch.should_process(50, Duration::from_millis(10)));
        assert!(batch.is_full(50));
    }
}

/// Analysis of operation types for adaptive processing
#[derive(Debug, Default)]
pub struct OperationAnalysis {
    pub create_count: usize,
    pub deposit_count: usize,
    pub withdrawal_count: usize,
}

/// Configuration for adaptive batch processing
#[derive(Debug, Clone)]
pub struct AdaptiveBatchConfig {
    pub base_batch_size: usize,
    pub locking_threshold: usize,
    pub max_batch_size: usize,
    pub min_batch_size: usize,
}

impl Default for AdaptiveBatchConfig {
    fn default() -> Self {
        Self {
            base_batch_size: 500,
            locking_threshold: 100,
            max_batch_size: 1000,
            min_batch_size: 50,
        }
    }
}

/// Bulk insert i≈ülemi sƒ±rasƒ±nda ge√ßici ayar deƒüi≈üikliklerini y√∂neten yapƒ±
pub struct BulkInsertConfigManager {
    original_event_store_config: Option<EventStoreConfig>,
    original_projection_config: Option<ProjectionConfig>,
    is_bulk_mode: bool,
}

impl BulkInsertConfigManager {
    pub fn new() -> Self {
        Self {
            original_event_store_config: None,
            original_projection_config: None,
            is_bulk_mode: false,
        }
    }

    /// Start bulk mode for EventStore only
    pub async fn start_bulk_mode_event_store(
        &mut self,
        event_store: &Arc<dyn EventStoreTrait>,
    ) -> Result<()> {
        if self.is_bulk_mode {
            return Ok(());
        }

        // Apply bulk config
        self.apply_bulk_event_store_config(event_store).await?;

        self.is_bulk_mode = true;
        info!("üöÄ EventStore bulk mode activated");
        Ok(())
    }

    /// Start bulk mode for ProjectionStore only
    pub async fn start_bulk_mode_projection_store(
        &mut self,
        projection_store: &Arc<dyn ProjectionStoreTrait>,
    ) -> Result<()> {
        if self.is_bulk_mode {
            return Ok(());
        }

        // Apply bulk config
        self.apply_bulk_projection_config(projection_store).await?;

        self.is_bulk_mode = true;
        info!("üöÄ ProjectionStore bulk mode activated");
        Ok(())
    }

    /// End bulk mode for EventStore only
    pub async fn end_bulk_mode_event_store(
        &mut self,
        event_store: &Arc<dyn EventStoreTrait>,
    ) -> Result<()> {
        if !self.is_bulk_mode {
            return Ok(());
        }

        // Restore original config
        if let Some(original_config) = self.original_event_store_config.take() {
            self.restore_event_store_config(event_store, original_config)
                .await?;
        }

        self.is_bulk_mode = false;
        info!("üîÑ EventStore bulk mode deactivated");
        Ok(())
    }

    /// End bulk mode for ProjectionStore only
    pub async fn end_bulk_mode_projection_store(
        &mut self,
        projection_store: &Arc<dyn ProjectionStoreTrait>,
    ) -> Result<()> {
        if !self.is_bulk_mode {
            return Ok(());
        }

        // Restore original config
        if let Some(original_config) = self.original_projection_config.take() {
            self.restore_projection_config(projection_store, original_config)
                .await?;
        }

        self.is_bulk_mode = false;
        info!("üîÑ ProjectionStore bulk mode deactivated");
        Ok(())
    }

    async fn apply_bulk_event_store_config(
        &mut self,
        event_store: &Arc<dyn EventStoreTrait>,
    ) -> Result<()> {
        // Event store'un any trait'ini kullanarak config'e eri≈üim saƒüla
        if let Some(event_store_impl) = event_store
            .as_any()
            .downcast_ref::<crate::infrastructure::event_store::EventStore>(
        ) {
            // Mevcut config'i kaydet
            let current_config = event_store_impl.get_config();
            self.original_event_store_config = Some(current_config);

            // Bulk config'i uygula (PostgreSQL ayarlarƒ± dahil)
            let _original_config = unsafe {
                let event_store_mut = event_store_impl
                    as *const crate::infrastructure::event_store::EventStore
                    as *mut crate::infrastructure::event_store::EventStore;
                (*event_store_mut).apply_bulk_config_with_postgres().await
            };

            info!("üìä Event store bulk config uygulandƒ±: batch_size=5000, processors=16, synchronous_commit=off (full_page_writes requires server restart)");
        }

        Ok(())
    }

    async fn restore_event_store_config(
        &self,
        event_store: &Arc<dyn EventStoreTrait>,
        original_config: EventStoreConfig,
    ) -> Result<()> {
        if let Some(event_store_impl) = event_store
            .as_any()
            .downcast_ref::<crate::infrastructure::event_store::EventStore>(
        ) {
            // Orijinal config'i geri y√ºkle (PostgreSQL ayarlarƒ± dahil)
            unsafe {
                let event_store_mut = event_store_impl
                    as *const crate::infrastructure::event_store::EventStore
                    as *mut crate::infrastructure::event_store::EventStore;
                (*event_store_mut)
                    .restore_config_with_postgres(original_config)
                    .await?;
            }

            info!("üîÑ Event store orijinal config geri y√ºklendi (PostgreSQL settings restored)");
        }

        Ok(())
    }

    async fn apply_bulk_projection_config(
        &mut self,
        projection_store: &Arc<dyn ProjectionStoreTrait>,
    ) -> Result<()> {
        // Projection store'un any trait'ini kullanarak config'e eri≈üim saƒüla
        if let Some(projection_store_impl) = projection_store
            .as_any()
            .downcast_ref::<crate::infrastructure::projections::ProjectionStore>(
        ) {
            // Mevcut config'i kaydet
            let current_config = projection_store_impl.get_config();
            self.original_projection_config = Some(current_config);

            // Bulk config'i uygula (PostgreSQL ayarlarƒ± dahil)
            let _original_config = unsafe {
                let projection_store_mut = projection_store_impl
                    as *const crate::infrastructure::projections::ProjectionStore
                    as *mut crate::infrastructure::projections::ProjectionStore;
                (*projection_store_mut)
                    .apply_bulk_config_with_postgres()
                    .await
            };

            info!("üìä Projection store bulk config uygulandƒ±: batch_size=2000, synchronous_commit=off (full_page_writes requires server restart)");
        }

        Ok(())
    }

    async fn restore_projection_config(
        &self,
        projection_store: &Arc<dyn ProjectionStoreTrait>,
        original_config: ProjectionConfig,
    ) -> Result<()> {
        if let Some(projection_store_impl) = projection_store
            .as_any()
            .downcast_ref::<crate::infrastructure::projections::ProjectionStore>(
        ) {
            // Orijinal config'i geri y√ºkle (PostgreSQL ayarlarƒ± dahil)
            unsafe {
                let projection_store_mut = projection_store_impl
                    as *const crate::infrastructure::projections::ProjectionStore
                    as *mut crate::infrastructure::projections::ProjectionStore;
                (*projection_store_mut)
                    .restore_config_with_postgres(original_config)
                    .await?;
            }

            info!(
                "üîÑ Projection store orijinal config geri y√ºklendi (PostgreSQL settings restored)"
            );
        }

        Ok(())
    }

    pub fn is_bulk_mode(&self) -> bool {
        self.is_bulk_mode
    }
}

impl Default for BulkInsertConfigManager {
    fn default() -> Self {
        Self {
            original_event_store_config: None,
            original_projection_config: None,
            is_bulk_mode: false,
        }
    }
}
