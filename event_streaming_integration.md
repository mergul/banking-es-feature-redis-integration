# Event Streaming Integration with Kafka

This document describes how Apache Kafka is integrated into the system for event streaming, facilitating communication between different parts of the application, particularly between the command side (where events originate) and the query side (where read models are updated).

## Role of Kafka

Kafka serves as a durable and scalable event bus. After domain events are generated by aggregates and successfully persisted in the Event Store (e.g., PostgreSQL), they are published to Kafka topics. This decouples the event producers from event consumers.

Key uses of Kafka in this architecture:
1.  **Propagating Domain Events:** Distributing events from the command side to various consumers.
2.  **Updating Read Models (Projections):** Consumers listen to event streams to build and maintain specialized query models.
3.  **Feeding Other Systems:** Events can be consumed by other microservices, analytics platforms, or monitoring tools.
4.  **Resiliency and Decoupling:** Acts as a buffer, allowing consumers to process events at their own pace and producers to operate even if some consumers are temporarily unavailable.

Relevant infrastructure components for Kafka integration include:
*   `src/infrastructure/kafka_abstraction.rs`: Provides a layer over the Kafka client.
*   `src/infrastructure/kafka_config.rs`: Manages Kafka connection configurations.
*   `src/infrastructure/kafka_event_processor.rs`: Likely handles the consumption and processing of events.
*   `src/infrastructure/kafka_dlq.rs`: Manages Dead Letter Queue for failed event processing.
*   `src/infrastructure/kafka_metrics.rs` & `src/infrastructure/kafka_monitoring.rs`: For observing Kafka client behavior.

## Event Publishing

1.  **Event Generation:** An aggregate processes a command and produces one or more domain events.
2.  **Persistence:** These events are atomically saved to the primary Event Store (e.g., the `events` table in PostgreSQL). This is the authoritative source of truth.
3.  **Publishing to Kafka:** After successful persistence, the events are published to a Kafka topic. This could be handled by:
    *   The Application Service that orchestrated the command.
    *   An "outbox" pattern, where a separate process reads newly committed events from the Event Store and reliably publishes them to Kafka. This ensures that events are published if and only if the transaction that saved them was successful. The `src/infrastructure/event_store.rs` might include logic for this, or a dedicated publisher service.

## Event Consumption

Various consumers subscribe to Kafka topics to react to events.

### Read Model Projections

*   A primary use case for event consumption is building and updating read models (projections).
*   Dedicated consumer services (potentially part of `src/infrastructure/projections.rs` or separate worker processes) subscribe to relevant Kafka topics.
*   When an event is received, the consumer updates its specific read model (e.g., a denormalized table in PostgreSQL or a document in Redis).
*   This ensures that the query side eventually reflects the changes made on the command side (eventual consistency).

### Kafka Topic Strategy

The choice of Kafka topics can vary:
*   **One Topic Per Aggregate Type:** E.g., `account_events`, `user_events`. This provides clear separation.
*   **A Single "Domain Events" Topic:** All events go to one topic, and consumers filter based on event type. This can be simpler to manage initially but might require more complex consumer-side filtering.
*   **Functional Grouping:** Topics based on bounded contexts or business functionality.

The specific strategy would be detailed in `src/infrastructure/kafka_config.rs` or related documentation.

## Data Flow Example

Here's a typical flow of data involving event streaming:

1.  **API Request:** A user action triggers an HTTP request (e.g., `POST /accounts/{id}/deposit`).
2.  **Command:** The Web Layer translates this into a Command (e.g., `DepositMoneyCommand`).
3.  **Application Handler:** An Application Service/Handler (in `src/application/handlers.rs`) receives the command.
4.  **Aggregate Processing:**
    *   The handler loads the relevant `Account` aggregate from the Event Store (`src/infrastructure/event_store.rs`).
    *   The command is processed by the aggregate, resulting in a new `MoneyDeposited` event.
5.  **Event Persistence:** The `MoneyDeposited` event is saved to the PostgreSQL Event Store.
6.  **Event Publishing:** The `MoneyDeposited` event is published to a Kafka topic (e.g., `account_events`) by the `src/infrastructure/kafka_abstraction.rs` or an outbox publisher.
7.  **Event Consumption & Projection Update:**
    *   A projection consumer (e.g., part of `src/infrastructure/projections.rs` or a separate service) listening to the `account_events` topic receives the `MoneyDeposited` event.
    *   The consumer updates a read model (e.g., an `account_balances` table in PostgreSQL or a document in Redis).
8.  **API Query:** A subsequent request (e.g., `GET /accounts/{id}/balance`) is served by querying this updated read model, providing a fast response.

## Error Handling and Resiliency

*   **Dead Letter Queues (DLQs):** As suggested by `src/infrastructure/kafka_dlq.rs`, if a consumer repeatedly fails to process an event, the event can be moved to a DLQ for later inspection and handling. This prevents a problematic event from blocking the processing of subsequent events.
*   **Idempotent Consumers:** Consumers should be designed to be idempotent, meaning processing the same event multiple times does not have unintended side effects. This is important for Kafka's at-least-once delivery semantics.
*   **Retry Mechanisms:** Consumers may implement retry logic for transient failures.
*   **Monitoring:** `src/infrastructure/kafka_metrics.rs` and `src/infrastructure/kafka_monitoring.rs` are crucial for observing the health and performance of the Kafka integration.

This integration of Event Sourcing, CQRS, and Kafka enables a decoupled, scalable, and resilient architecture where data flows reliably from state changes to various query models and downstream systems.
